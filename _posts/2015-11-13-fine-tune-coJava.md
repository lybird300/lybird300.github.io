---
layout: post
title: "Fine tune CoJava"
date: 2015-11-13
---
<blockquote>There is simply no substitute for the experience of writing and tuning your own parallel programs.</blockquote>
<img src="https://cloud.githubusercontent.com/assets/5496192/11194008/b9e11154-8c77-11e5-89ec-2d56b974c2d1.gif" /><br/>
Since CoJava applies Java fork-join parallelism framework, this post is mostly concerned with efficiently parallelizing fork-join computations in Java. The fork/join framework uses a thread pool in which a fixed number of threads are created. Each thread has a queue of tasks that are awaiting a chance to execute. When a task is started (forked), it is added to the queue of the thread that is executing its parent task. Because each thread can be executing only one task at a time, each thread's task queues can accumulate tasks which are not currently executing. Threads that have no tasks allocated to them will attempt to steal a task from a thread whose queue has at least one task - this is called work stealing. By this mechanism, tasks are distributed to all of the threads in the thread pool. By using a thread pool with work stealing, a fork/join framework can allow a relatively fine-grained division of the problem, but only create the minimum number of threads needed to fully exploit the available CPU cores. <b>Typically, the thread pool will have one thread per available CPU core</b>. 
There are some rules of thumb regarding general parallel programming (credit goes to the <a href="http://15418.courses.cs.cmu.edu/spring2015/home">course website of CMU 15-418/618</a>)
<ul>
<li>Want at least as much work as parallel execution capability (e.g., program should probably spawn at least was much work as there are cores)</li>
<li>Want more independent work than execution capability to allow for good workload balance of all the work onto the cores. “parallel slack” = ratio of independent work to machine’s parallel execution capability (in practice: ~8 is a good ratio)
<li>But not too much independent work so that granularity of work is too small (too much slack incurs overhead of managing fne-grained work)</li>
</ul>
The amount of work generated by a fork/join algorithm depends on a sequential cutoff, which is a certain condition (e.g., the size of an array to determine its maximum value is of size 1000) once the algorithm has reached it will switch from parallel processing to sequential processing (i.e., checking arrat elements one by one). When running time and resource intensive algorithms, it becomes important to fine tune the threshold against the system on which the algorithm is running and to determine the number of resources (e.g., CPU cores, memory size) that you should request and allocate during the process. Byt how? Let's first take a closer look at the fork-join framework.<br/>

<h2>More about Fork-Join</h2>
Fork-Join is usually used to handle recursive tasks. It was first introduced in the Java 7 and since then served its purpose well, probably because many large tasks in their nature can be represented as recursive. The central piece in the ForkJoin framework is a ForkJoinPool, which is an ExecutorService and thus can accept asynchronous tasks and return a Future object. With the Future object, you can keep track of the status of the computation. What makes ForkJoinPool different from other ExecutorServices is that its thread worker keeps checking out and "stealing" work from each other. Work stealing is a way to manage workload in a decentralized fashion with an attempt to improve efficiency.

<h2>Memory handling in Java</h2>
Java handles its memory in the heap and the stack. In the heap the Java Virtual Machine (JVM) stores all objects created by the Java application. The memory for new objects is allocated on the heap at run time. Instance variables live inside the object in which they are declared. Once your application have no reference anymore to an object (i.e., no other object refers to this object), the Java garbage collector of the JVM is allowed to delete this object and release the memory so that your application can use this memory again. If other objects still hold references to this object, then the garbage collector cannot release the memory. A memory leak occurs when object references that are no longer needed are unnecessarily maintained. <a href="https://en.wikipedia.org/wiki/Stack_(abstract_data_type)">Stack</a> is where the method invocations and the local variables are stored. If a method is called then its stack frame is put onto the top of the call stack. The stack frame holds the state of the method including which line of code is executing and the values of all local variables. The method at the top of the stack is always the current running method for that stack.<br/>
Each thread in a Java application has its own stack, which is used to hold return addresses, function/method call arguments, etc. When a thread intends to process large structures via recursive algorithms, it may need a large stack for all those return addresses and such. With Sun/Oracle JVM, you can set thread stack size via the parameter -Xss when you launch a java application, e.g., java -jar ...jar <b>-Xss</b>2000M -Xmx15g. Exactly what value you should assign need to be tuned/tested. Increase -Xss value if your keep getting StackOverflowError, which means the stack size is greater than the setting. Decrease the value if you get "OutOfMemoryError: unable to create new native thread", which may happen when -Xss is set too large, i.e., there are too many threads, each of which has a large stack. Note that this error is different from "OutOfMemoryError: Java heap space". The stack space used is not allocated from the heap (indicated by <a href="http://stackoverflow.com/questions/14763079/what-are-the-xms-and-xmx-parameters-when-starting-jvms">-Xms and -Xmx switches</a>) though. In fact the memory used by your JVM is more than what -Xmx parameter specifies. JVM uses more memory than just the heap. For example Java methods, <b>thread stacks</b> and native handles are allocated in memory separate from the heap, as well as JVM internal data structures. The default thread stack size varies with JVM (generally larger for 64bit JVMs), OS and environment variables. You can check your JVM using the command
<pre><code>java -XX:+PrintFlagsFinal -version | grep ThreadStackSize</code><pre>
Suppose the value is 512k, which means that if your app uses 150 threads, 75MB will be used for thread stacks. In some environments the defaults stack may be as large as 2MB. With a large number of threads, this can consume a significant amount of memory which could otherwise be used by your application or OS. The situation may become even more complicated when you are working on a cluster and need to decide how many nodes and cpus you should request. To monitor the resource usage of your program, you can log on to the cluster node (ssh username@computeNodeName) and then use "htop" command. So far I've only monitored the use of memory, swap (ideally 0), and <a href="http://blog.scoutapp.com/articles/2009/07/31/understanding-load-averages">CPU load</a>.<br/>

<h2>References</h2>
<ul>
<li><a href="http://www.vogella.com/tutorials/EclipseMemoryAnalyzer/article.html#memory">Eclipse Memory Analyzer (MAT) - Tutorial</a></li>
<li><a href="http://homes.cs.washington.edu/~djg/teachingMaterials/spac/grossmanSPAC_forkJoinFramework.html">Beginner's Introduction to Java's ForkJoin Framework</a></li>
</ul>
