---
layout: post
title: "CHAT"
date: 2015-08-24
---
Today I'm starting this post to record my thoughts while looking into Kirk's self-written software -- CHAT. Although I don't know (a) how often I will update, (b) how useful the content will be, or (c) how long it will take to complete the post, it is pretty exiting that after almost 5 months I'm finally here! ^_^

During the past 6+ years Kirk has been working on CHAT alone in his spare time. Now Charles and me are helping him publicize this software. To start with let me give you a brief introduction of CHAT. CHAT stands for Convergent Haplotype Association Tagging. It is a computer program that we are developing to detect rare moderate penetrant mutations from GWA data sets. <b>The core of this method is a graph-theory based algorithm that systematically searches for subsets of individuals that share long range haplotypes</b>. The underlying assumption is that a small (but significant) subset of unrelated individuals have the same disease because they have inherited a moderate penetrant mutation from a distant common ancestor. Since the algorithm is computationally intensive (an NP-complete problem), we have been cooperating with the Renaissance Computational Institute (RENCI) and using their high-performance computing clusters. How powerful these machines are? "Consider the resources are limitless." said Erik Scott, a Senior Research Software Developer at RENCI. 

Previous work showed the success of CHAT in constructed and simulated data sets as well as real data sets. The program has been applied in several independent projects to search for the genetic basis (i.e., causal mutations) of and treatment targets for diseases such as Parkinson's disease, stroke, alcoholism, multiple sclerosis, lymphoid cancer, and lung cancer and smoking. These studies found that CHAT is more likely to be successful with phenotypes that have lower population prevalence. The next step is to test CHAT using additional real data sets as they become available. Our goal is to help establish the baseline rate for detecting clusters of individuals with long shared haplotypes and to establish collaboration to resequence several genes for a few subjects. 

When I first looked into the code of CHAT, my impression is that Kirk put into a lot of efforts to make this software fault tolerant when working with clusters. CHAT tries to break up the jobs to use the number of CPUs that are expected. If a job fails it will be restarted and is often broken up into smaller jobs. CHAT keeps track of finished jobs and tries to resume remaining or failed ones. Most output files have internal line count information, so that a corrupted file can be automatically detected. Files are usually checked once for completeness but also frequently checked for whether they are where they are supposed to be. The above settings are accomplished through a properties file and can be changed in some in the longest step (i.e., phasing). Any step can be reinitialized (organized as multiple beans in the properties file), but then all subsequent steps then need to reinitialized. The intermediate data is stored in a derby database.

<h2>Install CHAT 1.0 -- for debuggers</h2>
CHAT is developed using eclipse and maven, so eclipse is the best environment for debugging. The executable file is chat.jar.
First install maven and eclipse. You need to have a home repository for maven . For example, mine looks like '/home/linly/.m2/repository'. Kirk's source code is stored in <a href="https://svn.renci.org/snv">renci SVN repository</a>. You can open the SVN repository perspective of Eclipse, log into the above address and checkout CHAT. Do not close Eclipse. Open an SSH session and 'cd' to wherever you put the checked out program, i.e., the checked-out chat root directory. For example, mine is '/projects/sequence_analysis/vol4/CHAT_simGWAS/workspace/chat-1.0/'. Run 'mvn clean install eclipse:eclipse' in the command line. Go back to Eclipse, refresh you project in Eclipse with f5. Then go to Window -> Preferences -> Java -> Build Path -> Classpath. Create a new variable called 'M2_REPO' and the value is <your home directory>/.m2/repository. For example, mine looks like '/home/linly/.m2/repository'. Then in Eclipse at the "Package Explorer" perspective, right click the root of the CHAT project and choose "import" from the drop-down menu. Import each of the sub directories (i.e., chat-common, chat-prep, chat-core, etc) as an existing project, so that they are all imported into the workspace.

Second, create a jar with all the dependencies includes (Note: with Eclipse Mars, this step could be done by building a Maven project). Type 'mvn clean install assembly:assembly' at the shell command line (make sure you are in the chat root directory). You will find the .jar file in the /target sub-directory of the root directory, probably named as "chat-1.0-SNAPSHOT-bin.jar". Copy this .jar file to a place where it can be found by the envrionment variable $Path (i.e, make sure it is "in the PATH"). For example, I keep all execution files/application programs in the /bin folder of my home directory (/home/linly/bin), so I can use something like
<pre><code>cp target/chat-1.0-SNAPSHOT-bin.jar ~/bin/chat.jar</code></pre>
Note that you need to make the .jar actually executable by
<pre><code>chmod +x chat.jar</code></pre>

<h2>CHAT inputs</h2>
In the the chat root directory there is a test example folder called 'TestDebug' that can be copied to a workspace as a test use case. This folder contains a genotype file that has just the data for a region on chromosome that contains the lrrk2 locus. 48 of the subjects in the pedigree file have a LRRK2 mutation that causes parkinsonsin's disease. The file lrrk_samples_save list the subjects. Not all of them are included in the analysis as CHAT excludes closely related individuals (I will introduce how this is done later). Before running the test example, make sure you have done the following:
<ul>
<li>Import (by right click and choose import from the drop-down menu) an existing project into workspace for each of the sub directories. (i.e., chat-common, chat-prep, chat-core etc). The classes built in these directories are what that are actually executed.</li>
<li>Open the 'chat.properties' file in the 'TestDebug' folder and change the path for the workdir to the new path. For example, I put the 'TestDebug' folder in '/projects/sequence_analysis/vol4/CHAT_simGWAS/workspace/chat-1.0', so the second line of the 'chat.properties' file is correspondingly "workdir=/projects/sequence_analysis/vol4/CHAT_simGWAS/workspace/chat-1.0/TestDebug".</li>
</ul>
The "TestDebug" folder contains the minimal input files CHAT needed (specified at the beginning of the "CHAT.properties" file). If you prefer your own test data, please also create and format these files (see below for detailed instructions). There is no need to name your files using same extensions (e.g., .lgen, .map, or .sample) though. You can simply use ".txt" or nothing. In the source code of CHAT (written in Java),  DataSet.java inside chat-common contains functions (readChatDataSet, readLgenDataSet) that are used to parse these input files. Now let me introduce these files one by one.
<ul>
<li>A raw genotype file (also referred to as the lgen file) with tab or comma delimited columns:<br/>
     <i>chromID,pos,rs,subjectID,genotype[,CONFIDENCE]</i><br/>
A lgen file corresponds to a single chromosome and each line in the file describes a subject's genotype of a particular marker SNP on that chromosome. The first five columns (required) provide detailed information on a marker: its located chromosome, physical position, <a href="http://www.ncbi.nlm.nih.gov/books/NBK44417/">rs number</a>, ID of the subject (individual), and genotype. The genotype data should be in single char format (e.g. 0,1,2 for AA, AG, GG) or <a href="http://www.bioinformatics.org/sms/iupac.html ">the IUPAC format</a> (e.g., A,R,G for AA, AG, GG). See the code file below for more information. The last column (optional) gives a float-type confidence value that can be used to filter the data. This file SHOULD be gzipped (end with .gz) to save space (required by the program). </li>
<li>A LDU map file: tab or comma delimited columns:<br/>
     <i>chromID,pos,rs,ldumap (-999999 for unknown place holder),in use bit</i></li>
The first three columns have the same meanings as in the lgen file. The fourth column (ldumap) indicates a particular marker's LDU (linkage disequilibrium unit) map position. The last column indicates whether or not this record will be used in later analysis: 0 means no use and 1 means use. 
<li>A sample file (also referred to as the fam file): tab or comma delimited SubjectID, DX (disease affection) {0 as unknown,1 as unaffected/control,2 as affected/case},Sex {2 female,1 male, 0 unknown},in use bit {0,1}</li> 
<li>A code file (also referred to as the truth table or truth matrix) indicates genotype coding strategy used in the data set. The first line gives the codes that represent homozygous geneotypes (e.g., A,C,G,T for AA, CC, GG, TT). The second line gives the codes of all possible genotypes (e.g., A,C,G,K,M,R,S,T,W,Y,X, if the IUPAC format is applied). These are also the horizontal and vertical elements of the truth table/matrix (referred to as truth vectors). Such a table checks compatibility. Regarding the IUPAC format, an "R" genotype can be either an "A" or a "G" genotype , i.e., an "R" is compatible with an "A" or a "G" (denoted as R = A or G). Thus, Y=C or T, S=G or C, W=A or T, K=G or T, M=A or C. In order to indicate that A is compatible with M, R, and X, "1" is assigned to the matrix cell AM, MA, AR, RA, AX, and XA. In contrast, "0" indicates incompatibility. Thus, a IUPAC code file should look like below (note that the placement of comma is critical for CHAT to correctly parse the truth matrix):
<pre><code>
,A,C,G,T
,A,C,G,K,M,R,S,T,W,Y,X 
A,1,0,0,0,1,1,0,0,1,0,1
C,0,1,0,0,1,0,1,0,0,1,1
G,0,0,1,1,0,1,1,0,0,0,1
K,0,0,1,1,0,1,1,1,1,1,1
M,1,1,0,0,1,1,1,0,1,1,1
R,1,0,1,1,1,1,1,0,1,0,1
S,0,1,1,1,0,1,1,0,0,1,1
T,0,0,0,1,0,0,0,1,1,1,1
W,1,0,0,1,1,1,0,1,1,1,1
Y,0,1,0,1,1,0,1,1,1,1,1
X,1,1,1,1,1,1,1,1,1,1,1
</code></pre></li>
</ul>

<h2>Play with CHAT -- Major modules/steps</h2>
<h3>CHAT preparation</h3>
The first step/module is to convert input files into CHAT accepted format. It is controlled by the CHAT_prep.xml file in the same folder. The file defines all the <a href="http://stackoverflow.com/questions/17193365/what-in-the-world-are-spring-beans">spring beans</a> used to accomplished this step. Users can skip a particular bean by commenting out the corresponding ref bean line in the threadList of the XML file using <!-- statement -->. <b>
Note: (a) In most of the steps called in CHAT.xml and CHAT_prep.xml there is a Main that can be edited so that the step can be run one at a time. I'll show you how to do that in details later.<br/>
(b) The step of building LDU map in this module is done by a third-party executable file called "ldmapper1", which you can find in the 'Notes_Resources' folder in the root directory. Move it to wherever convenient for you and add its path to the environment variable PATH, so that this file can be located. Also, building LDU map requires enough memory space, so you may want to grab a compute node of the cluster to run this module on. If you are going to run the module using Eclipse debug perspective, you need to grab an interactive node. Take a look at <a href="http://lybird300.github.io/2015/10/01/cluster-slurm.html">this post</a> on how to do that.

In Eclipse, to run using the debug perspective, go to run>debug_configerations. Right click on "java application" and a new Dialog box will pop up allowing you to create a new configuration for debug. Name the new instance CHAT_prep. CLick the browse button next to projects and select chat-commons. Next to the Main Class text box select Main-org.renci.chat. The change the project to chat-prep. (Main is in commons and chat-prep is dependent on it). Then click arguments tag. In the Program arguemnt type 'CHAT_prep.xml'; in the vm arguments you can add something like '-Xmx6g'. Change the working directory to wherever you have put the 'TestDebug' folder (At the bottome of the Arguments tab, there is a place allowing you to modify Working directory. Click on "Other" and specify the path). Click apply. Now you can click the 'Debug' button to start running the app or click the 'close' button to do this later.  
<h4>Step 1</h4>
The first three beans check the sanity of input files (users can scroll down in the "CHAT_prep.xml" file to see and modify the properties of each bean). "CheckLgenStructureRunnable" conducts sanity check on the lgen file. If there are any problematic genotype records (i.e., lines) in the file, the process will be terminated so that users can review detected problems, which are recorded in a "problemGenotypes.txt" file in the same folder. If no problem is detected, there won't be such a file (created and then deleted by the program). All "filtered" genotype records (obtained by stripping problematic and redundant lines) are outputted into a file named by adding a prefix "Filtered" to the the original sample lgen name. This file has the same format as the old file. If the user is willing to ignore (instead of trying to modify) all identified problem lines, he or she can set the "projectRawLongDataFileName" field in CHAT.properties (which is the field that indicates the lgen file to be used) to the file that contains all filtered genotype records (i.e., the one whose name starts with "Filtered"). Three other files can also be created during the process, although users can choose not to write any of them by commenting out corresponding property lines or setting the value to "" in the CHAT_prep.xml file. Among the three files, "GenotypeContainsChrom.txt" stores involved chromosomes (ordered by chromID), each per line. (b) "GenotypeContainsSamples.txt" stores involved subjects (ordered by subjectID), each per line with the format:<br/>
<i>subjectID,0 (place holder for unknown disease affection state),0 (place holder for unknown sex),1</i><br/>
(c) "GenotypeContainsSNPs.txt" stores involved marker SNPs (ordered by marker position), each per line with the format:<br/>
<i>chromID,pos,rsnumber,-77777.0 (place holder for unknown LDU map position),1</i><br/>

The second bean "CheckMapStructureRunnable" conducts sanity check on the LDU map file (required information and their number format and redundancy). If there are any problematic map elements in the file (one element as a line), the process will be terminated so that users can review detected problems (recorded in a "ProblemMapRecords.txt" file in the same folder). This bean mainly checks whether each record (i.e., each line) in the file has a unique rs number (the 3rd column/field), i.e., whether each SNP uniquely maps to a location. Two maps (data structure) are also created connecting each SNP (identified by its rs number) to its located chromosome (the 1st column/field) and physical position on that chromosome (the 2nd column/field) respectively. All "filtered" (i.e., non-problematic) map elements are sorted using their rs numbers and outputted into a file named by adding a prefix "FilteredSorted" to the old map file. This file has the same format as the old file. If the user is willing to ignore (instead of trying to modify) all identified problem lines, he or she can set the "projectMap" field in CHAT.properties (which is the field that indicates the map file to be used)  to the file that contains all filtered map records (i.e., the one whose name starts with "FilteredSorted"). Additionally, the bean can create a "MapContains<old map file name>.chrom" file listing all involved chromosomes (ordered by chromID), each per line (without redundancy). If users prefer, the bean also checks to see whether the map file contains all the SNPs in the "GenotypesContainsSNPs.txt" file outputted by the first bean (of course then that file must already exist). Any SNP in the previous file but missing in the map file will be outputted to a "SnpIdInGenotypeNotInMap.txt" file that has the same format as the previous file. If there are any SNPs mapped to more than one position or duplicate entries in the map, their information will be outputted to the screen  (i.e., standard output) and the process will be terminated.

The third bean "CheckFamStructureRunnable" conducts sanity check on the sample file (required information and their number format and redundancy). Any problematic sample element in the file (as a line) will be assigned -9 to its "include" "dx" and "sex" fields, so that it won't pass validity test. All problematic samples are outputted to a separate file named "problemSample.txt"; given no problem, this file will be automatically deleted. This bean mainly checks whether each record (i.e., each line) in the sample file has a unique sample ID (the 1st column/field). Duplicated sample lines are stripped and put in a file named by adding a prefix "dupSamples" to the original sample file name. All non-redundant sample elements are outputted into a file named by adding a prefix "FilteredSorted" to the old sample file; the list is sorted in the order of the "include" "dx" "sample ID" and "sex" fields subsequently. This file has the same format as the old file. If the user is willing to ignore (instead of trying to modify) all identified problem lines, he or she can set the "projectSamples" field in CHAT.properties (which is the field that indicates the map file to be used)  to the file that contains all filtered map records (i.e., the one whose name starts with "FilteredSorted"). If users prefer, this bean will check to see whether the sample file contains all the samples in the "GenotypesContainsSamples.txt" file outputted by the first bean. Missing samples will be outputted to a "SamplesIdInGenotypeNotInFamilyFile.txt" file that has the same format as the previous file but the "sex" and "dx" fileds of these sample records are changed to unknown and the "include" field to 0 (not included). Then the process will be terminated so that users can review these mismatched samples.

<h4>Step 2</h4>
Then the filtered input information is further checked, reorganized, and filtered. This is accomplished by two beans. The first bean "MakeChromSpecificChatFromLgen" collects information from the above output files and rearranges them for each chromosome. The records in the filtered lgen file are distributed into different txt files, each of which corresponds to one chromosome ("<chromID>.txt"), with every record (line) formatted as<br/>
<i>chromID,pos,rs number,subjectID,genotype,confidence</i>
<br/>. These txt files are stored in a sub-directory of the work directory (i.e., TestDebug) named "TempChromosomeSpecificRawData". If users are only interested in some but not all of the chromosomes in the data set, they can specify the chromosome(s) of interest beforehand by putting them in a separate file (see the "projectChromosomeList" field of the CHAT.properties file). Then only the information of specified chromosomes will be processed. Users can also filter the raw lgen records based on their QC scores (the "confidence" field). This bean also checks whether (a) every SNP has corresponding records in both the filtered lgen file and the filtered map file, and (b) each sample has a unique genotype at every spot (identified by its rs number) on one chromosome. In the latter case, given a single chromosome, there should not be multiple different records regarding a subject's genotype at the same spot. This kind of errors in the data set are referred to as "subject-snp discordant genotype" and will be outputted to a file named "Discord_<chromID>.txt" in the "chromosomeSpecificData" directory. In this file, each discordant subject-snp pair is described by three fields:<br/>
<i>subjectID,rs number of the SNP,occurrence of this subject-SNP pair in the data set</i>
When there is a discordant genotype, it will be coded as unknown (e.g., "X" in IUPAC). The major task of this bean is to use write input data files for CHAT. Each CHAT data file is converted from a previous "<chromID>.txt" file. Thus, it is also related with one chromosome and named after that chromosome ("<chromID>.txt"). The file starts with four lines: the first line aggregates all subject IDs separated by comma; the second line has all disease affect status (corresponding to subjects in the first line) separated by comma; the third line contains the "sex" field value of all subjects; the fourth line combines all "include" field values. Then are all SNP records, each per line in the form of<br/>
<i>chromID,pos,rs number,LDU map position,include or not (i.e., used or not),all subjects' genotypes of this SNP</i><br/>
The last field is a string of genotypes. After writing all CHAT data files to the "chromosomeSpecificData" folder, the program will delete the  "TempChromosomeSpecificRawData" directory to save space.

The second bean "FilterChromSpecificChat" applies four filters on CHAT data files (also referred to as chromosome specific files). Users can skip any of the filters by changing its corresponding value to a negative number or by commenting out the property line. Users can also specify the order in which these filters will be applied through the values of order properties (i.e., properties named as "***Order") in CHAT_prep.xml. Before filtering, this bean also conducts regular sanity check. Any SNP record (starting from the 5th line in a CHAT data file) with more or less than 6 fields (see above) will be considered invalid and read into the memory as the following<br/>
<i>"",-9,''invalid",-99999 (a predefined code meaning excluding this SNP from LDU map"),0 (i.e., not to be included)</i>
This bean also identifies and keeps records of SNPs and samples/subjects with more than 50% missing genotypes (IUPAC code "X") in the data set (referred to as "almost empty" SNPs and samples). The filtering introduced below are conducted on the rest of the data (samples and SNPs that have less than 50% missing genotypes). Now let's get to the "real business" of this bean, i.e., the four filters implemented by four functions. 
<ul>
<li>Hardy-Weinberg Equilibrium (not applicable for the X chromosome): For each SNP, the filter function first reads through samples to find out which codes (in the truth vector) represent the TWO homozygotes and one heterozygote regarding that SNP (denoted as a1, a2, and a12 in Java implementation; notably, the program assumes that all SNPs are biallelic. It will give an error message and stop once encountering a multi-allelic SNP). Then the filter function counts the occurrences of a1, a2, and a12 in all control samples (i.e., value of the "dx" field is not "2") and applies an exact test on the HWE of that SNP. The exact test for HWE has been mentioned in many articles, such as Emigh, T. H., 1980. A comparison of tests for Hardy-Weinberg equilibrium. Biometrics 36 627–642, or <a href="http://www.ncbi.nlm.nih.gov/pubmed/15789306">A note on exact tests of Hardy-Weinberg equilibrium</a>. If the test result cannot exceed a user-defined threshold, the SNP will be excluded from further analysis (set the "include" field to be 0)</li>
<li>Monomorphic SNPs: this filter function removes SNPs whose minor allele frequency (MAF) is smaller or equal to a user-defined threshold. A genotype code compatible with both a1 and a2 will be counted twice. (Note: the default MAF threshold is 0.01; since our simulated GWAS datasets contain only common variants (MAF >= 0.05) obtained using Haploview.tagger algorithm, all SNPs should be able to pass this filter.)</li>
<li>Missing SNPs: this filter function remove any SNP record (from the map attribute of a DataSet object) whose number of missing genotypes exceeds a user-defined threshold (by changing the "include" field from 1 to 0)</li>
<li>Missing samples: this filter function remove any sample record (from the sample attribute of a DataSet object) whose number of missing genotypes exceeds a user-defined threshold (by changing the "include" field from 1 to 0)</li>
</ul>
After the filtering, old chromosome specific data files (i.e., CHAT data files) are moved to a Backup subdirectory of the ChromosomeSpecificData directory. New CHAT data files will replace old corresponding ones (same name and format but with modified "include" fields; SNPs that have been filtered out will have include == 0) in this directory.

<h4>Step 3</h4>
The next step is to build an LDU map (a genetics linkage disequilibrium map measured in LD units (LDU), see <a href="http://lybird300.github.io/2015/04/21/gene-maps.html">one of my earlier post</a>) for every chromosome based on CHAT data files. Recall that each CHAT data file contains SNP information of one single chromosome, one SNP record per line starting from the 5th line. (Check how to build a LDU map in <a href="http://www.pnas.org/content/102/33/11835.full">this article</a>); only female samples are used to make X-chromosome LDU map). You can change names of the directories and files involved, but you have to do it consistently for all three beans introduced below that implement this step. At the time being we rely on an enternal executable program called "ldmapper1" written by <a href="http://www.southampton.ac.uk/medicine/about/staff/arc.page">Dr. Andrew Collins</a> to construct linkage disequilibrium (LD) maps. 

The first bean "makeLDMaps" intends to prepare and manage cluster-running jobs around this enternal software. Kirk used a database object (implemented by <a href="https://en.wikipedia.org/wiki/Apache_Derby">Aphache Derby (Java DB)</a> that supports <a href="https://en.wikipedia.org/wiki/Java_Database_Connectivity">JDBC</a> and <a href="https://en.wikipedia.org/wiki/SQL">SQL</a> as programming APIs) to store and manage all the LDU mapping jobs. He distinguished two types of Derby database objects in his code: one for LDU jobs (here) and the other for CHAT jobs (discussed later). The two databases will have different folders in the working directory, named as "lduDB" and "chatDB" respectively. See chat-common/src/main/java/org/renci/chat/derby/CHATDBApp.java for the sepcific SQL commands used for database initialization and management. The database for LDU jobs is named as "lduDB" and its contents are stored in a "lduDB" subfolder of the working directory. SQL statements are constructed as strings and executed for certain purposes. The program will break the entire chromosome into multiple segments and assign a job for each segment. To do so, an LDUJobsPlan table is built inside the database containing all these jobs; each record in the table has several fields such as job name, startMapIndex and endMapIndex of the chromosomal segment, number of SNPs on that segment (not every site is a SNP), and a field indicating whether the job is "done". The program allows users to define the values of several parameters:
<ul>
<li>What samples will be included in building the LDU map. Possible choices are "case", "control", "unknown", "case_control", "control_unknown", or "all". If users did not choose anything, the default strategy using by the program is "control", i.e., building the map on only control samples (no disease). If users did not use any of the above parameter values, the program will return an error message.</li>
<li>How long each chromosome segment and the overlapping area between two adjacent segements will be (by default 500 and 100 respectively). The size of the overlap should be smaller than the size of the segment</li>
</ul>
The program allows for at most three job running "cycles"; the next cycle will start only if the previous one fails. Each cycle will create its own folder for inputs (e.g., /LDUinput0) and outputs (e.g., /LDUmap0). Thus you will see /LDUmap0 and /LDUmap1 if the first cycle fails and the second one succeeds, or all three in the worst case. Inside the input folder, one subfolder will be made for each chromosome (named as "Chr_#", e.g., Chr_08). All jobs for a specific chromosome will be placed in this subfolder, each named as "Chr#_job#.txt.gz", e.g., 08_0001.txt.gz). The output file of a specific job will be placed in the output folder and the hierarchy is similarly organized. For example, the output file of the job /LDUinput2/Chr_03/03_0012.txt.gz is /LDUmap2/Chr_03/03_0012.txt.map). On planning a batch of jobs, the program will read the CHAT data file created in the previous step and the truth model, which will be used to parse the data file. The LDMappingJobs class will be used to create job info; you can find its source code at ../chat/common/LDMappingJobs.java. An important job property is the chromsomal segment to be processed in the job, delineated by the starting and the ending base pairs' map indices (startMapIndex and endMapIndex respectively). The program uses a "zigzag"-like approach to determine these two values for each job, as conseutive jobs share an overlap region (see the planInitalJobs function in ManageLDMapperRunnable.java). The input files, located at the input folder (e.g., /LDUinput#), are modified from the CHAT data file to include markers and samples with the "include" field = 1; in addition, the samples are selected based on predetermined sample inclusion strategy (by default only controls). The input files contain coded marker positions (as positions in a Kb physical map, for example, the marker at 123 will be coded as 0.123) and sample genotypes (as homozygote1, homozygote2, or heterozygote). The map building process is parallelized, with one thread created for processing one chromosome (corresponding input and output folders as arguments).
When the current cycle is done, the program will check the completeness of each job through its output file (.map). The major contents of this file is a table that maps markers within the segment region from their physical positions to LD map positions. The table has 5 columns indicating Line ID (1,2,3...) and then the name (as Chr#_physical position), physical poition (converted from integer to float by dividing it by 1000), LD map position (a continuous number), and MAF of a marker. The number of lines in the table is supposed to be the same as the number of markers in the segment, although the program allows for a small discrepancy. In other words, there can be a small number of markers missing and we still consider the job has been "completed". Complete jobs will be removed from the job database, while incompleted or missing ones will be redefined (to prepare for a rerun at the next cycle) and then added to the database in their new forms (use SQL update statements). Note that the program will count the number of updates so far and close the database every 1000 updates to "committ" these updates; otherwise the database will run more and more slowly (according to Kirk's experience). When a job is redefined, the new cycle ID will be attached to the job name and the chromosomal segment to be processed will be extended at both ends for half of the original segment size. The idea is to make a failed job bigger to cover more of the LDU map. We also assigned more running time to a job at a newer cycle (currently it proceeds in the order of 2 hrs, 4 hrs, and 24 hrs).

The second bean "readLDMapFilesConstructMap" will create a unified LDU map by combining individual LDU maps obtained from the previous bean. The program will produce a map data structure, of which the key is chromosome ID and the value is another map structure with the entry (physical position of a marker, a MapElement object containing all physical & LDU map information of this marker). Before executing this bean, the program will check whether there is an error file ("err.txt") in the working directory and notify the user if such a file exists. Basically, the user can choose to ignore the errors or stop to handle them. Recall that in the previous bean each job produces a separate LDU map based on a specific segment and the segments in consecutive jobs are overlapped. To make a unified LDU map, the program will find the middle point (called "transition") of an overlap (see ./chat-prep/...interpreter/ConstructLduMapRunnable.java/findTran() function). Then it uses the former job's results for the first half of the overlap (till the transition position) and the latter job's results for the second half (starting with the position right after the transition). Finding the middle point can be complicated, as the markers are not equally distributed along a segment and there can be missing data (so that "include" = 0). It is not an arbitrary decision that we break up overlap areas at their middle point, as middle points often show higher accuracy in estimating (what?). After re-defining the boundaries of individual LDU maps, overlaps may still exit but should be controlled < 10 markers (the program will check this and stop otherwise). Individual LDU maps created by different jobs have their own start and end points (different on physical positions but all originate from 0). On building the unified LDU map, we need to stich them together, so the LDU values of every marker on the next map need to be increased. The amount of increment can be calculated by focusing on the aforementioned "transition points", i.e., the new start points of the original jobs (except for the first job). The LDU increment of each marker on map j (j >= 2) is the LDU value of map j's start point on the previous map (i.e., map j-1) minus the LDU value of map j's start point on map j.

After obtaining the unified LDU map, the program will estimate the LDU positions of missing markers (their LDU values temporarily coded as -88888.0), if any. This process is called exptrapolation (see ../chat-common/.../chat/common/CHATUtil.java extrapolate() function) and it distinguishes between p and q arms: extrapolation on the p arm takes an ascending order while on the q arm takes a descending order. (All human chromosomes have 2 arms -- a short arm and a long arm -- that are separated from each other only by the centromere, the point at which the chromosome is attached to the spindle during cell division. By international convention, the short arm is termed the "p arm" while the long arm of the chromosome is termed the "q arm.") Ususally a chromosome is marked out from its p arm to its q arm on a genetic map; thus, genes on the p arm have smaller LDU values than genes on the q arm. Next, the program will normalizes the resultant unified map by making it start at zero: all markers will shift a certain amount of LDU distance x, which is the LDU value of the start point. Lastly, the program will check the resultant unified map, making sure the LDU positions of consecutive markers are indeed in an ascending order. The unified map will be output to the working directory in a file named "curLduMap.txt". Each line is a marker record with 5 comma-deliminated fields: chromID, physical position, rs name, ldu position, and whether included.

The last bean of this step is just to copy the CHAT data file (i.e., chromosome specific data file) and replace the last second column of each marker record with corresponding ldu map position of that marker.

<h4>Step 4</h4>
To be completed

<h3>CHAT main process</h3>
This process is controlled by CHAT.xml file and completed by 9 beans, while the chat.properties file continues to be used. The corresponding classes for these beans all inherit from an abstract class (/chat-core/src/main/java/org/renci/chat/interpreter/AbstractManageJobsRunnable), which defines common attributes of these beans and outlines a generic process of job management (runMethods()) including the following steps. Some of these steps correspond to abstract functions defined in the parent class (AbstractManageJobsRunnable.java) and overridden in each child class to fit bean-specific needs (as indicated in parentheses). After a step is finished, it returns a boolean value indicating whehter everything is fine and whether it is OK to proceed.
<ol>
<li>Get general job management settings from "chat.properties", such as the amount of memory requested for JVM (i.e., -Xmx, default 6g), the name of cluster queue requested, the number of cpu cores requested per node (default 3), the maximal number of jobs in the queue (default 200), target machine initial usage (default 512k?), the walltime for initial and resumed jobs (default increasing as 2, 4, 24 hours per job), and target Job size scale factor (default 1.0)</li>
<li>Check to see if the err.txt file exists and terminate the process if the file exists</li>
<li>Get chromosome specific directory path, truth code file, job database (chatDB) path, and the list of to-be-processed chromosome IDs
<li>Conduct bean-specific initialization steps to handle the transit between consecutive beans (uniqueStartSteps())</li>
<li>Connect to the database</li>
<li>Check whether there are any jobs that have been planned (checkIfPriorJobsPlanned()).b The program will query the "*JobsPlan" table of the CURRENT bean (again, each bean has such a table) to get a list of missing/undone jobs and a list of completed/done jobs (either can be empty). Currently if either list is not empty, the program will switch to the error recovery mode (set a boolean variable "errorRecoveryOn" to TRUE). (Later the missing job list will be checked and if it is empty the program will output messsage saying this step has been completed and then return from the current bean) The program will also query the "*JobsPlanSub" table of the PRIOR bean (each bean has such a table, see below) to get the names of completed/done jobs for each chromosome. Since the name of a job is the prefix of all job-related files including the final output file, it amounts to getting the names of all input files.<ul>
<li>If the error recovery mode keeps ON and the "reinitialize" boolean variable is FALSE (set this variable to TRUE if you are debugging each bean separately, as what is done in the main() function of each bean), it means that we are in a continuous sequential CHAT working process controlled by chat.xml and that the current bean was invoked before but not successfully completed. In other words, the current bean has some planned jobs that are not done, at least <b>according to the database records (no matter whehter involved files/directories exist)</b>. In this case, the next step for the program is to check the existence of the output directory of this specific bean and remake it only if it does not exit (makOutputDirs(false)). Then the program will check database records against files in the output directory and update the missing job list and the completed job list accordingly (checkForMissingJobs()). At this point, the content of each output file will be examined too using LSH Data standard(see below). Finally, incompatible database records will be corrected (markCompletedJobsAsDoneInDb()).</li>
<li>If the error recovery mode is OFF (i.e., not job records for the current bean in the database) or this step needs to be reintialized any way, the program will check the existence of input data directory (checkInputDirExist()) and input files (checkInputFilesExist()). As for the latter, the program will get a list of job names (as mentioned earlier) and go to the input data directory looking for files that start with these names and end with ".out.gz". Then the program will make an empty output directory of this specific bean (if existing, delete and remake) (makOutputDirs(true)), make the first "job plan" directory, i.e., ended with 0 (makeEmptyCurPlanDir(0)), plan initial jobs (delete this bean's previous job records in the database and make new records, in which jobs are marked as undone) (planInitialJobs()), create .properties and .xml files needed for job running (writeIncompleteJobs()) and eventually run all jobs in the aforementioned job plan directory (requiring the first setting of walltime) (runJobs(startHours[0])). All job-running .xml files are created using a template -- chat-core/src/main/resources/genericJob.xml.vm (AbstractManageJobsRunnable.java/makeJobXMLFile()). For each job, the runJobs() function creates a CHATJobsScriptRunnable object (/chat-common/src/main/java/org/renci/chat/common/CHATJobsScriptRunnable.java) and passes required information to it. This object writes a shell process for the specific job that calls a shell script named "runCHATv4.sh", which actually submits the job to your computing environment (i.e., the clusters of your institution). You can find an exemplary "runCHATv4.sh" script in the test case folder written for RENCI clusters. Inside this script you can find a command line as below
<pre><code>"java -jar $EXE $INFILE"</code></pre>
where $EXE is the chat.jar file, $INFILE is the aforementioned .xml file for a specific job.chat.jar integrates all classes needed to finish the entire CHAT main process and each of the nine beans will call its corresponding class (as indicated in .xml files of that bean's jobs).If executing this command line gives you an error message "no main manifest attribute in ../chat.jar", replace the above line with the following and retry.
<pre><code>"java -cp $EXE org.renci.chat.Main $INFILE"</code></pre>
The "runCHATv4.sh" script should be present in your working directory and executable (e.g., by "chmod +x runCHATv4.sh"). After a job is done, the corresponding CHATJobsScriptRunnable object will fetch the output, error message, and exit code of that job and prints them out with a job finish message.</li>
</ul></li>
<li>Next, the program will handle undone jobs from the previous running cycle. Right now there can be at most 3 cycles, each requires more walltime for each job than the previous cycle. The program will first call the current bean's checkForMissingJobs() and markCompletedJObsAsDoneInDb() functions. Then missing jobs (i.e., previously failed jobs) will be divided into smaller pieces (divideJobs()), because smaller jobs are more likely to get into the queue (doesn't it depend on the resource requirement? CHAT seems to make the same requirement...check the .xml file, but only if the dividing is possible -- some beans (e.g., ParalleLshDataCalcFilterRunnable) do not allow for dividided jobs. Failed jobs are marked in the database (markFailedJobs()). Then the program will make the next "job plan" directory, i.e., ended with current cycle + 1 (makeEmptyCurPlanDir(cycle + 1)), create .properties and .xml files needed for job running (writeIncompleteJobs()) and run all jobs in the aforementioned job plan directory (requiring the next setting of walltime) (runJobs(startHours[cycle])). This step will iterate until it uses up all cycles or all jobs are done (i.e., no missing jobs)</li>
<li>Indicate whether the current bean is executed successfully.</li>
</ol>
Each bean differs in their inputs, outputs, job size, and the database tables they use to store job information. As mentioned earlier, these specifications are set up by the uniqueStartSteps() function in each bean. Here, again, we use a derby database (with a folder named "chatDB" in the working directory) to store job information in multiple tables as below
<ul>
<li>The "RawLSHJobsPlan" table (for the first bean), the "MergeLSHJobsPlan" table (for the second bean), the "LSHAdjustJobsPlan" table (for the third bean), store job records with three fields: chromID, job name (as the prefix of job files), and whether the job is done (0--not done, 1--done).</li>
<li>The "RawLSHJobsPlanPos" table for the first bean stores the markers handled in each job; each record has two fields: job name, marker index (not marker physical position).</li>
<li>The "MergeLSHJobsPlanSub" table for the second bean has two fields: job name, sampleIndexSeed</li>
<li>The "LSHAdjustJobsPlanSub" table for the third bean has two fields: chromID, job name</li>
<li></li>

</ul>

The first bean "ParalleRawLshRunnable" puts job plans in folders named as "RawLSHDataPlan0/1/2". The input directory is "ChromosomeSpecificData" and the output directory is "RawLSHData". The max number of markers to be processed in each job is bounded and the bound is calculated by a formula: (int) Math.round((targetJobSizeScaleFactor * TARGET_RAW_JOB_SCALE_CONSTANT * effecencyScaleFactor) / ((Num_case + Num_control) * (Num_case + Num_control))), where by default TARGET_RAW_JOB_SCALE_CONSTANT = 2.5E7, and targetJobSizeScaleFactor = effecencyScaleFactor = 1. The LDU distance between two consecutive markers in each job is also bounded and the default distance is 3 ldu map units. For each job, the program will create a .properties file with the following information: chromID, whether it is an X chrom, output file name (for analysis results), indices of markers processed in this job, working directory, the location of chat data file (chromosome specific data directory), the name prefix of chat data file, the genotype coding file (i.e., truth matrix), the location of raw LSH data directory, forgiveCtBase, forgiveRateLength, assumedPriorProbIBD, postProbCutOffToEstimateProbIBD. Some of these items will be explained later. The first bean will call the class "LSHRawJobRunnable" to actually execute all its jobs. The code of this class is included in the same package as the first bean's execution class "ParalleRawLshRunnable.java". To see how the code works step by step, open "Debug configuration" in Eclipse, go to "Java Application" to create a new application (e.g., "LSHRawJobRunnable"). In the "Main" panel use what have been automatically detected (Kirk created a main function in each of these .java files). In the "Arguments" tab fill in the name of one of the job xml file in "RawLSHDataPlan#" folder. Open the LSHRawJobRunnable.java file, change Line 477 to indicate the same job's .properties file. Also insert a breaking point at the beginning of the main() function. Now we can lauch "LSHRawJobRunnable" in the debug mode.


The second bean "ParalleMergeRawLshRunnable" puts job plans in folders named as "LSHDataMergePlan0/1/2". The input directory is the output directory of the previous bean "RawLSHData" and the output directory is "LSHDataMerge". 

The third bean "ParalleLshDataAdjustRunnable" puts job plans in folders named as "LSHDataAdjustPlan0/1/2". The input directory is the output directory of the previous bean "LSHDataMerge" and the output directory is "LSHDataAdjust".

The fourth bean "ParalleLshDataCalcFilterRunnable" puts job plans in folders named as "LSHDataMergePlan0/1/2". The input directory is the output directory of the previous bean "LSHDataAdjust" and the output directory is "LSHDataMerge".

The fifth bean "CalculatePISMORThresholdsJobsRunnable"

The sixth bean "ParalleCHATSetRunnable"

The seventh bean "ParalleleLSHCHATSimulationRunnable"

The eighth bean "ParalleleLocalAdjustChatSetRunnable"

The last bean "ParalleleGenomeAdjustChatSetRunnable"
 
<h2>Weak points</h2>
CHAT can only handle biallelic SNPs (check the filterHWE function in chat/common/DataSet.java). How can we improve that? Is there a HW equilibrium testing method that accommodate multiallelic SNPs?

