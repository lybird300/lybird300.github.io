---
layout: post
title: "Multiple Comparisons: Bonferroni correction, FDR, and permutation testing"
date: 2015-04-01
---
Statistical tests are generally called significant and the null hypothesis is rejected if the p-value (the probability of seeing certain value of a specific test statistic provided that the null hypothesis is true) falls below a predefined alpha value, which is nearly always set to 0.05. This means that 5% of the time, the null hypothesis is rejected when in fact it is true and we detect a false positive (Type I error). Every statistical test comes with such a probability, but this probability is relative to one single test. In the case of GWAS, hundreds of thousands to millions of tests are conducted, each for a loci and with its own false positive probability. The cumulative likelihood of finding one or more false positives over the entire GWAS analysis is therefore much higher.

If we test 100,000 loci for association at the 5% level we should expect about 5,000 false positives. Thus, if we want the overall Type I error to remain at 5% we need to lower the significance level at each locus. There are several methods and I will focus on Bonferroni correction and False Discovery Rate (FDR) in this post. Bonferroni correction simply divides the significance level at each locus by the number of tests. In other words, it adjusts the alpha value from a = 0.05 to a =(0.05/k) where k is the number of statistical tests conducted. If the tests are independent then the Bonferroni bound provides a slightly conservative bound. If the tests are correlated then the bound becomes more conservative. Due to linkage disequilibrium among GWAS markers, it is generally untrue to assume that each association test on a GWAS data set is independent. Thus, applying Bonferroni correction often gives us the most conservative p-value threshold (maybe as a lower bound?) -- for a typical GWAS using 500,000 SNPs, statistical significance of a SNP association would be set as low as 1e-7. This is usually far too stringent and results in an enormous loss of power, producing many false negatives or Type II errors (failing to declare a test significant when the null is false).

Type I versus Type II tradeoff applies to most statistical tests (LWAppendix 5) and which error is more of a concern to the investigator determines how to proceed. In many cases, our initial experiment is simply an enrichment method: we wish to take a large number of possible hypotheses and extract a subset showing the most support for their alternative hypotheses for further consideration. In such cases, we are often more concerned with Type-II errors, as the penalty of including a test that is from the null may be less than the penalty of excluding a test that is not from the null.

One extension to Bonferroni correction is called sequential Bonferroni correction, which have increased power over standard Bonferroni correction. The general idea is that when we reject a hypothesis, there remain one fewer tests, and the multiple comparison correction should take this into account. A sequential Bonferroni correction method that allows for potential dependencies between tests was proposed by Holm (1979). Another method that intends to improve this dependency can be removed before individual tests are performed, and that removal of this dependency generates independent p values among the resulting tests.

An alternative to adjusting the false positive rate (alpha) is to determine the false discovery rate (FDR). The false discovery rate is an estimate of the proportion of significant results (usually at alpha = 0.05) that are false positives (i.e., the proportion of positive tests that are false). If we expect some reasonable number of the hypotheses to be false, then trying to avoid any false positives is not appropriate, but rather controlling the fraction of false positives in those tests we declare to be significant (discoveries) is a much better aim. This is especially true in large-scale exploratory experiments whose aim is to discover potential candidates for further studies. Under the null hypothesis that there are no true associations in a GWAS dataset, p values for association tests would follow a uniform distribution (evenly distributed from 0 to 1). Originally developed by Benjamini and Hochberg (1990; 1995), FDR procedurs essentially correct for this number of expected false discoveries, providing an estimate of the number of actual true results among those called significant. This technique has been widely applied to GWAS and extended in a variety of ways (van den Oord, 2008). Note that in this setting we attempt to control the false discovery rate (FDR), as opposed to the type I error (false positive) rate. The goal is to find a p-value threshold V such that the set of tests declared significant using p value <= V has the desired false discovery rate.

Given that the decision to control the false positives versus false discoveries hinges to a large extent on the fraction of the tests that are true nulls, we often need to estimate this fraction from the empirical distribution of the p values. To get such distribution, we often rely on permutation testing. Permutation testing is another general approach for establishing significance in GWAS (not just for estimating the distribution of p values). While somewhat computationally intensive, permutation testing is a straightforward way to generate the empirical distribution of test statistics for a given dataset when the null hypothesis is true. This is achieved by randomly reassigning the phenotypes of each individual to another individual (i.e., swapping the trait labels among the subjects) in the dataset, effectively breaking the genotype-phenotype relationship while keeping the LD structure of the dataset. Each random reassignment of the data represents one possible sampling of individuals under the null hypothesis, and this process is repeated a predefined number of times N to generate an empirical distribution with resolution N, so a permutation procedure with an N of 1000 gives an empirical pvalue within 1/1000th of a decimal place. Several software packages have been developed to perform permutation testing for GWAS studies, including the popular PLINK software, PRESTO, and PERMORY.

Another commonly used approach is to rely on the concept of genome-wide significance. Based on the distribution of LD in the genome for a specific population, there are an "effective" number of independent genomic regions, and thus an effective number of statistical tests that should be corrected for. For European descent populations, this threshold has been estimated at 7.2e-8 (Dudbridge & Gusnanto, 2008). This reasonable approach should be used with caution, however, as the only scenario where this correction is appropriate is when hypotheses are tested on the genome scale. Candidate gene studies or replication studies with a focused hypothesis do not require correction to this level, as the number of effective, independent statistical tests is much, much lower than what is assumed for genome-wide significance.

<h2>References</h2>
<ul>
<li>Benjamini and Hochberg (1995) Controlling the false discovery rate: a practical and powerful approach to multiple testing. JRSS(B) 57:289-300</li>
<li>Dudbridge F, Gusnanto A (2008) Estimation of significance thresholds for genomewide association scans. Genet Epidemiol 32: 227–234</li>
<li>Hochberg Y, Benjamini Y (1990) More powerful procedures for multiple significance testing. Stat Med 9: 811–818.</li>
<li>van den Oord EJ (2008) Controlling false discoveries in genetic studies. Am J Med Genet B Neuropsychiatr Genet 147B: 637–644.</li>
<li>Walsh & Lynch (to be published in fall, 2015). Evolution and Selection of Quantitative Traits: I. Foundations (Version 26 March, 2014). A4. Multiple comparisons: Bonferroni Corrections and False Discovery Rates</li>
</ul>
