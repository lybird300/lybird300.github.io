---
layout: post
title: "CHAT"
date: 2015-11-30
---
<blockquote></blockquote>

<h2>CHAT main process -- Part 1</h2>
This process is controlled by CHAT.xml file (as opposed to CHAT_prep.xml) and completed by 9 beans, while the chat.properties file continues to be used. Each bean differs in their inputs, outputs, job size, and the database tables they use to store job information. Here, again, we use a derby database (with a folder named "chatDB" in the working directory) to store job information in multiple tables:
<ul>
<li>The "RawLSHJobsPlan" table (for the first bean), the "MergeLSHJobsPlan" table (for the second bean), the "LSHAdjustJobsPlan" table (for the third bean), store job records with three fields: chromID, job name (as the prefix of job files), and whether the job is done (0--not done, 1--done).</li>
<li>The "RawLSHJobsPlanPos" table for the first bean stores the markers handled in each job; each record has two fields: job name, marker index (not marker physical position).</li>
<li>The "MergeLSHJobsPlanSub" table for the second bean has two fields: job name, sampleIndexSeed</li>
<li>The "LSHAdjustJobsPlanSub" table for the third bean has two fields: chromID, job name</li>
<li></li>
</ul>
Despite their differences, most of these beans have their execution classes (indicated by <bean id="" class=""> in CHAT.xml) inherited from the same abstract class (/chat-core/src/main/java/org/renci/chat/interpreter/AbstractManageJobsRunnable), which defines common attributes of these beans and outlines a generic process of job management (runMethods()) including the following steps. Some of these steps correspond to abstract functions defined in the parent class (AbstractManageJobsRunnable.java) and overridden in each child class to fit bean-specific needs (as indicated in parentheses). After a step is finished, it returns a boolean value indicating whehter everything is fine and whether it is OK to proceed.
<ol>
<li>Get general job management settings from "chat.properties", such as the amount of memory requested for JVM (i.e., -Xmx, default 6g), the name of cluster queue requested, the number of cpu cores requested per node (default 3), the maximal number of jobs in the queue (default 200), target machine initial usage (default 512k?), the walltime for initial and resumed jobs (default increasing as 2, 4, 24 hours per job), and target Job size scale factor (default 1.0)</li>
<li>Check to see if the err.txt file exists and terminate the process if the file exists</li>
<li>Get chromosome specific directory path, truth code file, job database (chatDB) path, and the list of to-be-processed chromosome IDs
<li>Conduct bean-specific initialization steps to handle the transit between consecutive beans (uniqueStartSteps())</li>
<li>Connect to the database</li>
<li>Check whether there are any jobs that have been planned (checkIfPriorJobsPlanned()).b The program will query the "*JobsPlan" table of the CURRENT bean (again, each bean has such a table) to get a list of missing/undone jobs and a list of completed/done jobs (either can be empty). Currently if either list is not empty, the program will switch to the error recovery mode (set a boolean variable "errorRecoveryOn" to TRUE). (Later the missing job list will be checked and if it is empty the program will output messsage saying this step has been completed and then return from the current bean) The program will also query the "*JobsPlanSub" table of the PRIOR bean (each bean has such a table, see below) to get the names of completed/done jobs for each chromosome. Since the name of a job is the prefix of all job-related files including the final output file, it amounts to getting the names of all input files.<ul>
<li>If the error recovery mode keeps ON and the "reinitialize" boolean variable is FALSE, it means that we are in a continuous sequential CHAT working process controlled by chat.xml (and every relevant move leaves a record in chatDB) and that the current bean was invoked before but not successfully completed. In other words, the current bean has some planned jobs that are not done, at least <b>according to the database records (no matter whehter involved files/directories exist)</b>. In this case, the next step for the program is to check the existence of the output directory of this specific bean and remake it only if it does not exit (makOutputDirs(false)). Then the program will check database records against files in the output directory and update the missing job list and the completed job list accordingly (checkForMissingJobs()). At this point, the content of each output file will be examined too using LSH Data standard(see below). Finally, incompatible database records will be corrected (markCompletedJobsAsDoneInDb()). [Debugging tip: If you only want to debug a single bean, do this via its main() function, which will set the "reinitialize" variable to be TRUE. Any bean can be reinitialized in this way. If you are debugging the entire process and would like to rerun a bean, you cannot just modify CHAT.xml file by commenting out all previous beans, because every relevant move of the process has left a record in chatDB and any unmatching will raise a flag that causes the program to stop. If you want to restart from a specific bean, in addition to comment out previous beans, you should also change the value of the "reinitialize" property of that bean from "false" to "true" in CHAT.xml. </li>
<li>If the error recovery mode is OFF (i.e., not job records for the current bean in the database) or this step needs to be reintialized any way, the program will check the existence of input data directory (checkInputDirExist()) and input files (checkInputFilesExist()). As for the latter, the program will get a list of job names (as mentioned earlier) and go to the input data directory looking for files that start with these names and end with ".out.gz". Then the program will make an empty output directory of this specific bean (if existing, delete and remake) (makOutputDirs(true)), make the first "job plan" directory, i.e., ended with 0 (makeEmptyCurPlanDir(0)), plan initial jobs (delete this bean's previous job records in the database and make new records, in which jobs are marked as undone) (planInitialJobs()), create .properties and .xml files needed for job running (writeIncompleteJobs()) and eventually run all jobs in the aforementioned job plan directory (requiring the first setting of walltime) (runJobs(startHours[0])). All job-running .xml files are created using a template -- chat-core/src/main/resources/genericJob.xml.vm (AbstractManageJobsRunnable.java/makeJobXMLFile(); import org.apache.velocity.*). For each job, the runJobs() function creates a CHATJobsScriptRunnable object (/chat-common/src/main/java/org/renci/chat/common/CHATJobsScriptRunnable.java) and passes required information to it. This object writes a shell script and starts a running process for each specific job. During the process, another shell script named "runCHATv4.sh" will be executed to actually submit the job to your computing environment (i.e., the clusters of your institution). You can find an exemplary "runCHATv4.sh" script in the test case folder written for RENCI clusters and inside this script you can find a command line as below
<pre><code>"java -jar $EXE $INFILE"</code></pre>
where $EXE is the chat.jar file, $INFILE is the aforementioned .xml file for a specific job.chat.jar integrates all classes needed to finish the entire CHAT main process and each of the nine beans will call its corresponding class (as indicated in .xml files of that bean's jobs). If executing this command line gives you an error message "no main manifest attribute in ../chat.jar", replace the above line with the following and retry.
<pre><code>"java -cp $EXE org.renci.chat.Main $INFILE"</code></pre>
The "runCHATv4.sh" script should be present in your working directory and executable (e.g., by "chmod +x runCHATv4.sh"). After a job is done, the corresponding CHATJobsScriptRunnable object will fetch the output, error message, and exit code of that job and prints them out with a job finish message. Job outputs and error messages are collected using the StreamGobbler class (in the same package as CHATJobsScriptRunnable), because the Java library used to run job processes tend to crash when there accumulated too many outputs/error messages.</li>
</ul></li>
<li>Next, the program will handle undone jobs from the previous running cycle. Basically, it will divide undone jobs into a set of subjobs, each with fewer subjects (not sure..). Right now there can be at most 3 cycles, each requests more walltime for each job than the previous cycle. The program will first call the current bean's checkForMissingJobs() and markCompletedJObsAsDoneInDb() functions. Then missing jobs (i.e., previously failed jobs) will be divided into smaller pieces (divideJobs()), because smaller jobs are more likely to get into the queue (doesn't it depend on the resource requirement? CHAT seems to make the same requirement...check the .xml file, but only if the dividing is possible -- some beans (e.g., ParalleLshDataCalcFilterRunnable) do not allow for dividided jobs. Failed jobs are marked in the database (markFailedJobs()). Then the program will make the next "job plan" directory, i.e., ended with current cycle + 1 (makeEmptyCurPlanDir(cycle + 1)), create .properties and .xml files needed for job running (writeIncompleteJobs()) and run all jobs in the aforementioned job plan directory (requiring the next setting of walltime) (runJobs(startHours[cycle])). This step will iterate until it uses up all cycles or all jobs are done (i.e., no missing jobs). Jobs are named in a certain way to indicate the connections between an initially planned job and all its counterparts in later cycles. (Note: Kirk has rewrote this part of code, so that the program can figure out these connections through slurm jobID...) [Debugging tip] To suit the need of a specific bean, you can adjust the walltime requirement in each cycle as well as the number of cycles via the "startHoursPerJobString" property of that bean in CHAT.xml. If there are undone jobs after all cycles (the program will notify you and write an err.txt file into the working directory), detele the err.txt file, relaunch CHAT making sure "reintialize" is FALSE (see above), the program will pick up undone jobs and try to rerun ONLY these jobs through a new iteration of cycles.]</li>
<li>Indicate whether the current bean is executed successfully.</li>
</ol>

The first bean "ParalleRawLshRunnable" puts job plans in folders named as "RawLSHDataPlan0/1/2". These jobs utilize input data from the "ChromosomeSpecificData" directory and output results to the "RawLSHData" directory. The jobs created by this bean will be executed by another class "LSHRawJobRunnable" in the same package as this one (i.e., "ParalleRawLshRunnable"). You will see this separation of a "job creator" and a "job runner" in the next several beans and it is the job runner class that does the real trick. To see how a job runner class (e.g., "LSHRawJobRunnable") works step by step, open "Debug configuration" in Eclipse, go to "Java Application" to create a new application (e.g., "LSHRawJobRunnable"). In the "Main" panel use what have been automatically detected (Kirk created a main function in each of these .java files). In the "Arguments" tab fill in the name of one of the job xml file in "RawLSHDataPlan#" folder. Open the LSHRawJobRunnable.java file, change Line 477 to indicate the same job's .properties file. Also insert a breaking point at the beginning of the main() function. Then we can lauch "LSHRawJobRunnable" in the debug mode. Below I will introduce different job creater and runner classes as they are subsequently invoked in the CHAT main process controlled by CHAT.xml. I would like to start with the big picture, which hopefully can help with the navigation in case we dive too deep. As shown in the figure below, the main CHAT process consists of the following steps
<ol>
<li>Identify potential long shared IBD haplotypes</li>
<li>Order potential haplotypes by piSMOR metric</li>
<li>Phase each individuals</li>
<li>Transitive Testing determines the size of the clique –if person A and person B share a haplotype and person B and person C share a haplotype, do person A and person C share a haplotype</li>
<li>Form Cliques</li>
<li>Correct for local Sharing</li>
<li>Permutation testing: gene dropping – to remove false sharing – significance testing – genotyping error rates and the number of generations that have passed since the founder lived</li>
</ol>

Now let's look into the first job creator "ParalleRawLshRunnable". It determines job size based on marker density. The maximal number of markers to be processed in each job is bounded by (int) Math.round((targetJobSizeScaleFactor * TARGET_RAW_JOB_SCALE_CONSTANT * effecencyScaleFactor) / ((Num_case + Num_control) * (Num_case + Num_control))), where by default TARGET_RAW_JOB_SCALE_CONSTANT = 2.5E7, and targetJobSizeScaleFactor = effecencyScaleFactor = 1. In addtiion, the LDU distance between two consecutive markers in each job is also bounded and the default distance is 3 ldu map units. For each job, the program will create a .properties file with the following information: chromID, whether it is an X chrom, output file name (for outputting analysis results), indices of markers processed in this job ("listIndexPos"), working directory, the location of chat data file (chromosome specific data directory), the name prefix of chat data file, the genotype coding file (i.e., truth matrix), the location of raw LSH data directory, forgiveCtBase, forgiveRateLength, assumedPriorProbIBD, postProbCutOffToEstimateProbIBD. Some of these items will make more sense when I introduce the corresponding job runner class "LSHRawJobRunnable". 
"LSHRawJobRunnable" detects IBD haplotypes from every pair of samples/subjects (i.e., pairwise comparison; order does not matter) through every marker position assigned to a job ("listIndexPos" in the job property file). The IBD probability of two individuals is positively correlated with the number of consecutive corresponding markers on which the two indiviudals have compatible genotypes (diplotype, not haplotype data). The genotype data are coded using the truth table mentioned earlier, by which their compatibility is determined too. As you can see, what matters here is the length of sharing/compatibility/similarity, so IBD segments are referred to as <b>long shared haplotypes (LSH)</b> in this bean and the rest of CHAT beans. A potential obstacle in estimating IBD, especially concerning recent demography, is that random sequencing and phasing errors will tend to break up LSH. Thus, CHAT allows for certain amount of accumulated genotyping error before it stops checking the compatibility/similarity of the rest markers (on the chromosome segment to be processed in a job). More specifically, accumulated genotype error is bounded by a threshold that considers both how many incompatible sites have been accumulated so far ("forgiveCtBase" in the property file) and how fast they are accumulated ("forgiveRateLength" in the property file). For any pair of individuals, the LHS identifying process starts at a given marker and works towards two sides. On one side (called "p" side), markers are checked in a descending order interatively (markerID--) until it hits the beginning of this chromosome segment or the accumulated number of errors has achieved the predefined threshold. On the other side (so-called "q" side), markers are checked in an ascending order (markerID++) until it hits the end of this chromosome segment or the accumulated number of errors has achieved the predefined threshold (same threshold as the p side). The checking process returns an SampleToSampleGenotypeConparisonDataElement object (also referred to as sample-to-sample CHAT bean) that stores the result of one comparion between two individuals that starts from a particular marker (i.e., information on an LHS identified via the comparsion), including the pair of individuals being compared (their IDs), the starting marker index, the pair's disease affecting status (0 - both controls, 1 - one case one control, 2 - both cases), the pair's gender (indicated by specific values defined in /chat-common/src/main/java/org/renci/chat/common/CHATUtil.java, setPairClass() function), the smallest marker ID that has been checked, the biggest marker ID checked, and LDU map distance between these two markers. The program creates a CHATSimilarityMatrixData object for each marker, storing all SampleToSampleGenotypeConparisonDataElement objects (related to different pairs of individual samples) obtained from comparisons starting at that marker, along with that marker's physical position and the number of case-case, case-control & control-control pairs involved in those comparison.<br/>
Since the number of pairs increases quadratically with the number of individuals, pairwise comparisons on a big data set (due to many samples or high marker density) may take a long time and the similarity matrix can be huge. Thus, more efficient strategy is needed to process the data. Common ones include breaking down the original chromosome segment and/or the sample set (into subsets). CHAT uses an algorithm that first tries to build a null model -- the distribution of IBS segments around a marker -- using genotype data of control indiviudals and then uses the fitted distribution to identify IBD segments, which are supposed to fall into the tail area of the distribution. More specifically, to estimate the IBD probability of every detected shared haplotype, the program will first fit a <a href="https://en.wikipedia.org/wiki/Gamma_distribution">gamma distribution</a> to the length (measured by LDU map distance) of all shared haplotypes around a marker, for all the markers. <a href="https://en.wikipedia.org/wiki/Distribution_fitting">This wiki page</a> explains why we do so. The empirical data CHAT uses to fit the distribution are retrieved from aforementioned CHATSimilarityMatrixData objects. Each marker has such an object storing information on LSHs detected around that marker. A CHATSimilarityMatrixData object contains a list of SampleToSampleGenotypeConparisonDataElement objects (i.e., CHAT beans), each of which records the LDU distance of haplotypes shared by a specific pair of individuals (by default they should both be controls). As long as there are totally more than 200 data points for a marker (each data point is the LDU distance of a shared haplotype obtained from one single pairwise comparison), we can go ahead fitting the length distribution of LSH around that marker. 
CHAT implements <a href="http://lybird300.github.io/2015/11/11/IBD-Bayesian.html">a Bayesian fitting</a> process that has several parameters to be predefined and/or adjusted over iterations (they will make more sense in later discussion):
<ul>
<li>the prior IBD probability ("assumedPriorProbIBD"), that is, the chance that any detected LSH is due to IBD. It is current a constant with default value 0.005. Alternatively, we could estimate it from the data <span style="color:#FF0000">[future improvement]</span>.</li>
<li>the threshold of posterior IBD probability ("postProbCutOffToEstimateProbIBD"). If an observed LSH has a posterior IBD probability exceeds this threshold, CHAT considers it as due to IBD (default is 0.9, which means 10% false positive).</li>
<li>the probability threshold CHAT uses to exclude extreme LSH from the Gamma distribution ("extremeProbExcludeNullDist"). Gamma distribution is a partially bounded distribution that theoretically extends from 0 to plus infinity. While we intend to fit a Gamma distribution for IBS segments, they are inseparable from IBD segments in the data. An extreme long segment is more likely to be IBD rather than IBS, and therefore should be excluded to prevent it from distorting the fitted distribution. The value of this parameter is calculated by 1-(1/100*numOfIncludedControls), which is something like 0.99..9.</li>
</ul>
Bayesian fitting proceeds iteratively. In each iteration, the program filters out "suspicious" IBD segments (with IBD probability > "extremeProbExcludeNullDist") and uses the remaining data points fit the gamma distribution for IBS segments, until there are no more "suspicious" IBD segments. Thus, the "extremeProbExcludeNullDist" threshold determines the number of iterations. The gamma distribution fitted at the last iteration will be used for future computation. Distribution fitting applies a third-party gamma-distribution fitting function <a href="http://simul.iro.umontreal.ca/ssj/doc/html/umontreal/iro/lecuyer/probdist/GammaDist.html#getInstanceFromMLE(double[], int)">umontreal.iro.lecuyer.probdist.GammaDist.getInstanceFromMLE(double[] c, int n_c)<a>, c being the set of data points used to estimate α and λ, and n_c being the size of this data set. Gamma distribution has different forms of parameterization. This function creates a new instance of the following form of gamma distribution with shape parameter α > 0 and scale parameter λ > 0 (these two parameters will also get more precise through iterations).
Density function: <img src="https://cloud.githubusercontent.com/assets/5496192/11452159/320ef3fc-95ac-11e5-94bf-2228f25c9526.png" />
You can tell from the name of the function that it uses maximum likelihood method to fit the Gamma distribution (i.e., to figure out the most likely values of parameters α and λ). As mentioned in the above wiki page, there are several alternative ways, yet MLE is often the most efficient method in the sense that results are as precise as is possible for a given amount of data. 
Using the fitted gamma distribution and the prior IBD probability ("assumedPriorProbIBD"), we can calculate the IBD probability of each input LSH based on its length as follows. Assume S is a variable indicating the length of LSH (i.e., the number of contiguous markers identical alleles). We would like to infer the probability that an observed LSH of length c is IBD, i.e. P(IBD│S>c) =?
<pre><code>
Use Baye's theorem, P(IBD|S>c) = P(IBD & S>c)/P(S>c)
Since P(IBD & S>c) is indeed P(IBD), we have
P(IBD|S>c) = P(IBD)/P(S>c)
</code></pre>
P(IBD) is the assumed prior probability that any detected LSH is due to IBD. Denote P(IBD) = Z and P(S>c|notIBD) = Y.
<pre><code>
Since P(IBD) + P(notIBD) = 1, P(S>c|IBD) = 1, we have
P(S>c) = P(S>c|IBD)*P(IBD) + P(S>c|notIBD)*P(notIBD) = 1*Z + Y*(1-Z) = Z + Y*(1-Z)
P(IBD|S>c) = P(IBD)/P(S>c) = Z/(Z+Y*(1-Z))
</code></pre>
As mentioned, Z = 0.005. Since Y = P(S>c|IBS), 1 - Y is P(S<=c|IBS), which is the value of the cumulative density function (CDF) of the fitted Gamma distribution, P(S<=X|IBS), when X = c. CHAT estimates the CDF using the<a href="http://simul.iro.umontreal.ca/ssj/doc/html/umontreal/iro/lecuyer/probdist/GammaDist.html#cdf(double)">cdf</a> function in the same Java library mentioned earlier. Now for an input LSH of length X, current gamma distribution gives its posterior IBD probability P(IBD|S>X), which is then compared with aforementioned posterior IBD probability threshold ("postProbCutOffToEstimateProbIBD") to determine whether this LSH should be treated as IBD. <span style="color:#FF0000">[fugure improvement: if we allow prior IBD probability to self-adjust over iterations, its value at iteration t+1 = (number of estimated IBD LSHs + number of extremely long LSHs)/total number of detected LSHs at iteration t. The denominator is equivalent to the total number of pairwise comparisons]</span>
The length threshold used to filter out extremely long shared haplotypes (and thus likely IBD) is conveniently set as Double.MAX_VALUE at the first iteration, so no segments are actually excluded at all. In each of later iterations (2nd, 3rd,...t-th), this value will be recalculated based on the gamma distribution fitted during the previous iteration (1st, 2nd,...(t-1)th respectively), so that it can actually do something. The calculation is shown below.
<pre><code>
Recall that we have predefined the PROBABILITY threshold for exclusion ("extremeProbExcludeNullDist") to be 0.9...9. This value is by essence P(IBD|S>X). Now we need to figure out the LENGTH threshold X. Denote P(IBD|S>X) = Q, we have
Q = Z/(Z+Y*(1-Z))
Y = (Z/Q-Z)/(1-Z)
Recall that 1-Y gives P(S<=X|IBS), the cumulative density function (CDF) of the fitted Gamma distribution. So we can calculate X by putting 1-Y into the inverse function of the CDF, which CHAT gets through the<a href="http://simul.iro.umontreal.ca/ssj/doc/html/umontreal/iro/lecuyer/probdist/GammaDist.html#inverseF(double)">inverseF</a> function in the same Java library as earlier mentioned 
</pre></code> 
The fitting process is conducted by the FitCHATSharingDistributionGamma class (chat-common/src/main/java/org/renci/chat/fitChatSharingDistribution/FitCHATSharingDistributionGamma.java). Using marker-specific gamma distributions fitted at the last iteration, CHAT re-calculate the posterior IBD probability of every detected LSH. If the probability > 0.5 (by default; you can find the default values of most aforementioned parameters either in chat.properties file or in chat-common/java/org.renci.chat.Constant.java), that LSH will be considered as IBD. Results of the fitting process are organized by three classes: LSHData, LSHDataElement and FittingData (all in the package /chat-common/src/main/java/org/renci/chat/common). An LSHData object corresponds to a job planned by ParalleRawLshRunnable and contains two lists. One list are LSHDataElement objects each storing infomration of an LSH around a marker (including the posterior IBD probability of each LSH) for all markers handled in the job. The other list are FittingData objects each storing information of a Gamma distribution fitted for a marker. Each job also has an output file in the output directory (../RawLSHData/Chr_#/filename) with the following content.
<pre><code>
row 1: chromosome	"number of fitted gamma distributions"	"number of LSH"
row 2-N: parameters of fitted gamma model, one per row
row N+1-M: long shared haplotypes, one per row with the following fields
0: lshid
1: indexmapseed (index on the LDU map)
2: indexsample1 (ID of the first subject involved in the pairwise comparison)
3: indexsample2 (ID of the second subject involved in the pairwise comparison)
4: subject pair class (0 control-control, 1 case-control, 2 case-case)
5: sex pair class (1=Male-Male, 2=Female-Female, 3=Male-Female 0=unknown-?)
6: indexmapp (the end of the LSH on the p arm)
7: indexmapq (the end of the LSH on the q arm)
8: ldu (LDU map distance of the LSH)
9: isvalid
10: p value (posterior IBD probability of the LSH)
11: fixindexp (at this step its value is -9)
12: fixindexq (at this step its value is -9)
13: pismore (at this step its value is -9.0)
</code></pre>
The second bean "ParalleMergeRawLshRunnable" puts job plans in folders named as "LSHDataMergePlan0/1/2". The input directory is the output directory of the previous bean "RawLSHData" and the output directory is "LSHDataMerge". This class calls "LSHMergeRawForSubJobRunnable" to run all the planned jobs, merging the LSHs created in the first bean that overlap and have the same sample ids. this merges the lsh from that start at  points into a single lsh







<a href="http://www.mathwave.com/articles/extreme-value-distributions.html">this post</a>

It then estimates the (posterior) IBD probability of the haplotype. 



	



The third bean "ParalleLshDataAdjustRunnable" puts job plans in folders named as "LSHDataAdjustPlan0/1/2". The input directory is the output directory of the previous bean "LSHDataMerge" and the output directory is "LSHDataAdjust". This class calls "LSHDataAdjustJobRunnable" to run all the planned jobs, trimming the ends of each LSH. trims off the end a bit assuming that part of the end that could be shared is actually due to chance

The fourth bean "ParalleLshDataCalcFilterRunnable" puts job plans in folders named as "LSHDataFilterPlan0/1/2". The input directory is the output directory of the previous bean "LSHDataAdjust" and the output directory is "LSHDataFilter". This class calls "LSHDataCalcFilterJobRunnable" to run all the planned jobs, calculating metrics about LSH including PI smores. calculates metrics about LSH including PI smores

The fifth bean "CalculatePISMORThresholdsJobsRunnable" calculates statistics for PiSmore: ldu floor, median, mean, sd. These are stored to the database. calculates means & sd of sharing of LSH

The sixth bean "ParalleCHATSetRunnable" calls "BuildChatSetRunnable" to help with its jobs. "ParallelCHATSetRunnable" builds regions over which LSH cliques will be formed, while "BuildChatSetRunnable" computes cliques for a region. does a long iterative process to decide which strand different lsh are sharing with subjects. make graphs of LSh at many spots for several models. Models allow for greater error and min pismore or LDU. gives first peek if anything interesting. Significance called at cliques for many points. Results are collapsed into representative results.

The seventh bean "ParalleleLSHCHATSimulationRunnable" calls "LSHCHATSimulationJobRunnable" to help with its jobs. "ParalleleLSHCHATSimulationRunnable" constructs the list of simulations to run, while "BuildChatSetRunnable" runs each simulation and computes fisher statistic. Permutes phenotypes for cliques and calculates staticistics

The eighth bean "ParalleleLocalAdjustChatSetRunnable" This class calls "LSHCHATLocalAdjustJobRunnable" to run all the planned jobs, correcting for local excess sharing. 

The last bean "ParalleleGenomeAdjustChatSetRunnable" This class calls "LSHCHATGenomeAdjustJobRunnable" to run all the planned jobs, correcting for size of genome for each model and Corrects for multiple models (?)
 
<h2>Weak points</h2>
CHAT can only handle biallelic SNPs (check the filterHWE function in chat/common/DataSet.java). How can we improve that? Is there a HW equilibrium testing method that accommodate multiallelic SNPs?

As mentioned, CHAT aims for long shared haplotypes (LSH) with error tolerence and puts LSHs into cliques based on their indiviudal hosts. A clique can include too many and too long haplotypes and therefore incur extensive computing time, given either of two conditions:
<ul>
<li>the tolerance for genotyping errors is set too high</li>
<li>there are too few recombinations or mutations along the chromosome (when the data set is synthetic, it means the simulation model that generates the data set has inappropriate recombination or mutation rate)</li>
</ul>
