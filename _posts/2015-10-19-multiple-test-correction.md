---
layout: post
title: "The multiple comparison problem in GWAS: Bonferroni correction, FDR control, and permutation testing"
date: 2015-10-19
---
<blockquote>If you are unfamiliar with some terms mentioned in this post, Wikipedia is a good place for quick references. I myself find direct<a href="https://en.wikipedia.org/wiki/Familywise_error_rate#Classification_of_m_hypothesis_tests"> mathematical definitions</a> most helpful.</blockquote>
Statistical tests are generally called significant and the null hypothesis is rejected if the p-value (the probability of seeing certain value of a specific test statistic provided that the null hypothesis is true) falls below a predefined alpha value, which is nearly always set to 0.05. This means that 5% of the time, the null hypothesis is rejected when in fact it is true and we detect a false positive (Type I error). Every statistical test comes with such a probability, but this probability is relative to one single test. "Multiple comparisons" or "multiple testing" is the name attached to the general problem of making decisions (e.g., considering a set of statistical inferences simultaneously) based on the results of more than one test. The nature of the problem is made clear in <a href="http://stats.stackexchange.com/questions/88065/explain-the-xkcd-jelly-bean-comic-what-makes-it-funny">this post</a> (the XKCD "Green jelly bean" cartoon). With regard to GWAS, in a typical study there are hundreds of thousands to millions of tests simultaneously conducted, each for a single marker and with its own false positive probability. The cumulative likelihood of finding one or more false positives (i.e., false associations) over the entire GWAS analysis is therefore much higher. For example, testing 100,000 loci for association at the 5% level is expected to have about 5,000 false positives. If we want the overall Type I error (i.e., <a href="https://en.wikipedia.org/wiki/Familywise_error_rate#The_FWER">family-wide error rate</a>) to remain at 5% we need to lower the significance level at each locus. 

There are a variety of methods that serve this purpose. The first group includes some <a href="https://en.wikipedia.org/wiki/Familywise_error_rate#Controlling_procedures">classic controlling procedures</a>, represented by Bonferroni correction. Bonferroni correction simply divides the significance level at each locus by the number of tests. In other words, it adjusts the alpha value from a = 0.05 to a =(0.05/k) where k is the number of statistical tests conducted. If the tests are independent then the Bonferroni bound provides a slightly conservative bound. If the tests are correlated then the bound becomes more conservative. Due to linkage disequilibrium among GWAS markers, it is generally untrue to assume that each association test on a GWAS data set is independent. Thus, applying Bonferroni correction often gives us the most conservative p-value threshold (maybe as a lower bound?) -- for a typical GWAS using 500,000 SNPs, statistical significance of a SNP association would be set as low as 1e-7. This is usually far too stringent and results in an enormous loss of power, producing many false negatives or Type II errors (failing to declare a test significant when the null is false).

Type I versus Type II tradeoff applies to most statistical tests and which error is more of a concern to the investigator determines how to proceed. The goal of Large-scale exploratory experiments is usually to select from original data a reduced set of potential candidates for further analysis. To include all possible true alternatives (as opposed to true nulls), reseasrchers are often willing to accept some false positives. In many GWASs, we wish to take a large number of possible hypotheses and extract a subset showing the most support for their alternative hypotheses for further consideration. It is also known that GWASs suffer from low power (i.e., susceptible to Type II errors) when the object(s) of study are rare variants, rare diseases, or variants with small effects. Thus, we are often more concerned with Type-II errors, as the penalty of including a test that is from the null may be less than the penalty of excluding a test that is not from the null.

Sequential Bonferroni correction is a power-increasing extension to standard Bonferroni correction. The general idea is that when we reject a hypothesis, there remain one fewer tests, and the multiple comparison correction should take this into account. A sequential Bonferroni correction method that allows for potential dependencies between tests was proposed by Holm (1979). Bonferroni corrections and their extensions are appropriate when we expect only a few of the many null hypotheses being false. Assume n is the total number of tests and n0 is the number of true nulls, taht means n0 is close to n. An alternate setting is that some substantial fraction of the tests are indeed expected to be false. In such cases, even sequential Bonferroni correction are likely too stringent, resulting in too many false negatives (i.e., less power). If we expect some reasonable number of the hypotheses to be false, it would be more appropriate to control the fraction of false positives in those tests we declare to be significant (i.e., false discoveries) than to bound false positive rate across all tests including those whose null hypotheisis we cannot reject. That being said, we also don’t want to be completely swamped with false positives, so the fraction of false positives within the reduced set still needs to be controlled. 

A method resulting from this line of thinking is to control false discovery rate (FDR) -- the proportion of significant results that are false positives (i.e., the proportion of positive tests that are false). This method tries to find a p-value threshold tao such that the set of tests declared significant using p value <= tao has the desired false discovery rate delta (delta = 5% means that on average 5% of the genes we declare as being significant are actually not). Originally developed by Benjamini and Hochberg (1995), FDR procedures essentially correct for the number of expected false discoveries, providing an estimate of the number of actual true results among those called significant. This technique has been widely applied to GWAS and extended in a variety of ways (van den Oord, 2008). The original Benjamini-Hochberg FDR Estimator

As mentioned, the decision to control the false positives versus false discoveries hinges to a large extent on the fraction of the tests that are true nulls (n0/n), which is usually unknown. Some earlier works are thus relevant to the topic of this post due to their attempts to estimate n0 (for the purpose of rejecting the global null hypothesis -- all tests are from their respective true null hypotheses). We know that draws of p values follow a uniform distribution over (0,1) if the null is correct. Otherwise, if the collection of tests contains some alternative hypotheses mixed in with true nulls, the distribution tends to be a mixture, with fraction n0/n being draws from a uniform and (1 - n0/n) from some other distribution. Some approaches were developed to help people visually detect the discrepancy/departure. One of them is the Schweder-Spjøtvoll plot in which p values are ordered from smallest p(1) to largest p(n) and their rank r(i) is plotted as a function of 1-p(i). Under a uniform distribution, the result is a straight line passing through the origin and the point (1, n), as the upper curve shown below. When alternative hypothesis (as opposed to null) is correct, there will be an inflation of p values near zero and therefore a strong departure from linearity near one in the lower curve (p near zero, so 1 - p values near one).<br/>
<img src="https://cloud.githubusercontent.com/assets/5496192/10640021/e83543e6-77e0-11e5-8eb3-420e8e21d7c0.PNG" width="80" /><br/>
Alternatively, we can plot out the empirical distribution of p values (using a histogram) and see if there is any departures from a uniform distribution. A uniform distribution results in a flat histogram, whereas any shift will lead to a skew towards 0 or 1. The methods just introduces also allow to infer n0 from the plots they create, as reviewed Appendix 4 of Walsh and Lynch's book (see References)

The multiple comparison problem in GWAS is more complicated than that in a general setting for two reasons -- the large scale of simultaneous comparisons and the correlations among these comparion tests. Contemporary genetic association studies may test hundreds of thousands to millions of genetic variants for association, often with multiple binary and/or continuous traits or under more than one model of inheritance. Many of these association tests may be correlated with one another because of linkage disequilibrium between nearby markers and correlation between traits and models. And don't forget that all these tests are conducted on one common data set (i.e., the same set of individuals) that may not have a large number of samples. As mentioned, a multiple-comparison problem makes statistical inferences based on the joint distribution of p-values from all the tests. If these tests are independent, it is relatively easy to compute the joint distribution of p-values assuming the null hypothesis of each test is true (each p value is a random value following a uniform distribution). If these tests are dependent, however, the calculation becomes much more difficult because the null distribution of p values can significantly depart from a uniform.

So how are the methods we have introduced doing in this context? How are their effectiveness and efficiency impacted? When tessts are correlated, Bonferroni correction is overly conservative. To give an extreme example, when all the p-values are the same (as in a case of perfect dependence), the cutoff value for the Bonferroni procedure (assume it is alpha/number of markers) should be just alpha. The original FDR controlling procedure assumes independence among multiple tests (Benjamini and Hochberg, 1995). Dependence in the noise of the statistics creates problems only for deriving FDR-controlling procedures and therefore can be addressed (Benjamini and Yekutieli, 2001), as long as the dependence structure meets certain constraint. (see <a href="http://stats.stackexchange.com/questions/111756/the-meaning-of-positive-dependency-as-a-condition-to-use-the-usual-method-for">this post</a>). However, the large-scale dependence in GWAS is not statistical but genetic and creates problems in the applicability of the FDR quantity itself (Chen and Storey, 2006). First of all, it is difficult to interpret FDR in GWAS. or significance measures such as the FDR that take into account the number of “true discoveries,” it is necessary to implement some interpretable measure of the unit of signal that is not easily manipulated. This appears to be rather difficult in GWAS, where it is often not possible to determine the actual number of causal variants. Moreover, it is common practice to call several markers significant -- all associated with a common trait. This does not appear to be taken into account in the current formulation of FDR for GWAS. Thus, applying FDR across multiple correlated loci (due to LD) for a single trait is dubious.


It is therefore of interest to account for the true dependence structure of the p-values (or the individual test statistics) in order to derive more powerful controlling/correctiong procedures. Statistically this can be achieved by applying resampling methods such as bootstrapping and permutations methods, admitted that they are computationally intensive. Genetically, we can model the dependency among the SNPs (due to LD) and incorporating the information into previous methods/procedures (a derivative of Holm's method for multiple comparison (3), Hidden Markov Model (4), conditional or positive FDR (5) or derivative thereof (6)).

Permutation testing provides a valide adjustment of p-values for the multiple comparisons problem. It can generate the empirical distribution of test statisticsassuming the null hypothesis is true while maintaining the original correlation structure. A typical permutation test on GWAS results is achieved by randomly reassigning the phenotypes of each individual to another individual (i.e., swapping the trait labels among the subjects) in the dataset, effectively breaking the genotype-phenotype relationship while keeping the LD structure of the dataset. Each random reassignment of the data represents one possible sampling of individuals under the null hypothesis, and this process is repeated a predefined number of times N to generate an empirical distribution with resolution N, so a permutation procedure with an N of 1000 gives an empirical pvalue within 1/1000th of a decimal place. Randomly permuting and reanalyzing the data many times and comparing the permutation-based results with the original results allows estimation of the probability of observing a P value as in the original result, given the correlation between tests. This solution is attractive because of its simplicity and robustness and is often considered the gold standard for analysis. However, in the context of GWAS, permutation is likely to require too much computation time, so computationally efficient alternatives are desirable. Several software packages have been developed to perform permutation testing for GWAS studies, including the popular PLINK software, PRESTO, and PERMORY. Usually, the plink software can give you raw and permuted p-values, although it uses (by default) an adaptive testing strategy with a sliding window that allows to stop running all permutations (say 1000 per SNP) if it appears that the SNP under consideration is not "interesting"; it also has option for computing maxT, see the online help.


Another group of methods is to extend the Bonferroni or FDR adjustments to account for the correlation between tests. When multiple tests (say M) are correlated, the true probability of observing a particular P value in a test is smaller because there is less variation between test statistics than if the tests were independent, which makes extreme test statistics less likely. In effect, it is as though fewer tests were performed; for this reason, an intuitive solution is to estimate and utilize the effective number of independent tests (M_effs), as they are the tests that should be corrected for. For example, some researchers suggest calculating the effective number of independent SNPs in a gene region and using this value in the Bonferroni correction (Cheverud 2001; Dudbridge & Gusnanto, 2008; Galwey 2009; Gao et al., 2008; Li and Ji, 2004; Nyholt 2004). This approach is considered less conservative than the basic Bonferroni correction and more computationally efficient than permutation testing, but the accuracy is unsatisfactory when this approach was applied in GWAS (Dudbridge & Koeleman 2004; Salyakina et al., 2005). After all, relying on a single parameter cannot fully capture a correlation structure as complex as genome-wide LD of SNPs. A further method uses extreme tail theory to explicitly calculate the probability of detecting an extreme value (i.e., beyond predefined thresholds) of the test statistic (Conneely & Boehnke 2007).




So-called gap statistics or sliding window have been proved successful in some case, but you'll find a good review in (7) and (8). I've also heard of methods that make effective use of the haplotype structure or LD, e.g. (9), but I never used them. They seem, however, more related to estimating the correlation between markers, 

Another method that intends to improve this dependency can be removed before individual tests are performed, and that removal of this dependency generates independent p values among the resulting tests.

<h2>References</h2>
<ul>
<li>Benjamini and Hochberg (1995) Controlling the false discovery rate: a practical and powerful approach to multiple testing. JRSS(B) 57:289-300</li>
<li>Benjamini, Y., and D. Yekutieli, 2001. The control of the false discovery rate under dependence. Ann. Stat. 29: 1165–1188.</li>
<li>Broberg, P. A comparative review of estimates of the proportion unchanged genes and the false discovery rate. BMC Bioinformatics 2005 6: 199.</li>
<li>Chen, Lin, and John D. Storey. "Relaxed significance criteria for linkage analysis." Genetics 173.4 (2006): 2371-2381.</li>
<li>Cheverud JM: A simple correction for multiple comparisons in interval mapping genome scans. Heredity 2001; 87: 52–58.</li>
<li>Conneely KN, Boehnke M: So many correlated tests, so little time! rapid adjustment of p values for multiple correlated tests. Am J Hum Genet 2007; 81: 1158–1168.</li>
<li>Dalmasso, C, Génin, E and Trégouet DA. A Weighted-Holm Procedure Accounting for Allele Frequencies in Genomewide Association Studies. Genetics 2008 180(1): 697–702.</li>
<li>Dudbridge F, Koeleman BP (2004) Efficient computation of significance levels for multiple associations in large studies of correlated data, including genomewide association studies. Am J Hum Genet 75:424–435<li>
<li>Dudbridge F, Gusnanto A (2008) Estimation of significance thresholds for genomewide association scans. Genet Epidemiol 32: 227–234</li>
<li>Galwey NW: A new measure of the effective number of tests, a practical tool for comparing families of non-independent significance tests. Genet Epidemiol 2009; 33: 559–568.<li>
<li>Gao X, Starmer J, Martin ER: A multiple testing correction method for genetic association studies using correlated single nucleotide polymorphisms. Genet Epidemiol 2008; 32: 361–369.<li>
<li>Hochberg Y, Benjamini Y (1990) More powerful procedures for multiple significance testing. Stat Med 9: 811–818.</li>
<li>Li J, Ji L: Adjusting multiple testing in multilocus analyses using the eigenvalues of a correlation matrix. Heredity 2005; 95: 221–227.<li>
<li>Nyholt, DR. A Simple Correction for Multiple Testing for Single-Nucleotide Polymorphisms in Linkage Disequilibrium with Each Other. Am J Hum Genet. 2004 74(4): 765–769.</li>
<li>Salyakina D, Seaman SR, Browning BL, Dudbridge F, Mu¨ller-Myhsok B (2005) Evaluation of Nyholt’s procedure for multiple testing correction. Hum Hered 60:19–25</li>
<li>van den Oord EJ (2008) Controlling false discoveries in genetic studies. Am J Med Genet B Neuropsychiatr Genet 147B: 637–644.</li>
<li>Walsh & Lynch (to be published in fall, 2015). Evolution and Selection of Quantitative Traits: I. Foundations (Version 26 March, 2014). A4. Multiple comparisons: Bonferroni Corrections and False Discovery Rates</li>
<li>Wei, Z, Sun, W, Wang, K, and Hakonarson, H. Multiple Testing in Genome-Wide Association Studies via Hidden Markov Models. Bioinformatics 2009 25(21): 2802-2808.</li>
<li><a href="http://stats.stackexchange.com/questions/2819/correcting-p-values-for-multiple-tests-where-tests-are-correlated-genetics">Correcting p values for multiple tests where tests are correlated (genetics)</a></li>
</ul>
