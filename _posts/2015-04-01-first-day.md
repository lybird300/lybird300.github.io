---
layout: post
title: "Polimorphism, genealogy, and simulation"
date: 2015-04-01
---
Modern geneticists make sense out of DNA sequences using POLYMORPHISM DATA. These data include the genotypes of many individuals sampled at one or more loci; a locus is considered polymorphic if two or more distinct types of sequences are observed, regardless of their frequencies. The goal of much modern genetic research is to find gene variations that contribute to disease. Finding these genes should allow an understanding of the disease process, so that methods for preventing and treating the disease can be developed.

<b>Why do we  care about not onlySNPs but also the variations in haplotypes?</b>

A SNP is a site in the DNA where different chromosomes differ in the base they have. For example, 30 percent of the chromosomes may have an A, and 70 percent may have a G. These two forms, A and G, are called variants or alleles of that SNP. An individual may have a genotype for that SNP that is AA, AG, or GG. Haplotypes are the allelic configuration of multiple genetic markers that are present on a single chromosome of a given individual.

For diseases with a relatively straightforward genetic basis, the single-gene disorders, current methods are usually sufficient to find the genes involved. Most people, however, do not have single-gene disorders, but develop common diseases such as heart disease, stroke, diabetes, cancers or psychiatric disorders, which are affected by many genes and environmental factors.

Moreover, discoveries on SNPs (single nucleotide polymorphisms) will be most useful if their organization along chromosomes, the haplotype structure, is known. 

<b>Why do we care about the geneaological causes to gene variations?</b>


<p>Population and evolutionary genetics study evolutionary forces or ancient events that create and maintain genetic variation. At the heart of it is the use of gene genealogy, a tree structure describes the evolutionary history (e.g., divergence) of a particular set of DNA sequences and their relatedness. The shape of the genealogy depends on genetic events such as inheritage, mutations, recombination, as well as population events such as isolation, migration, population growth, etc. geneticists have been using it to infer or predict DNA sequence variation. Due to the uncertainty elements of reproduction, the central part of genealogical data analysis is a stochastic characterization of the genealogies that relate the sequences.

Geneticists use two basic approaches: linkage analysis, which can be also described as identity by descent, and association analysis, which can be described identity by state. Linkage analysis looks to see if the same chromosome is inherited by the descendants of an individual that have the same trait. Association analysis looks to see if the specific gene variations are more common among people with a trait. In this example an individual can have an A or G nucleotide at a specific DNA position. One can see that the affected individuals are more likely to have a G than an A. The G nucleotide is said to be associated with being affected. 








Modern genetics studies use data collected from natural populations. This approach has two problems. Firstly, there is no replication of the ‘experiment’, only one run of evolution is available to be studied. Secondly, the starting conditions of the ‘experiment’ are unknown. However, statistical hypothesis testing expects and bears on inherent uncertainty/randomness of observations. Imagine that we sequence a 10-kb region in 30 randomly chosen individuals and, surprisingly, find no polymorphisms. We might interpret this observation as evidence for selective constraint in this region. Alternatively, it might be that the individuals chosen for the comparison are unusually closely related. So, the interpretation depends on the genealogy of the sequences, which is not known. One solution is to model and simulate the past using a suitable stochastic model. To decide if the above data are unusual, we might make assumptions about the process that gave rise to those data, and imagine many random repetitions of the evolutionary process. If the fraction of the random genealogical and mutational histories that could have given rise to the observed data is small, we can conclude that the assumptions cannot explain the pattern. To consider genealogies that might be found in different runs of evolution, we need models that allow us to construct random genealogies as a result of certain evolutionary forces.


Consider a sample of haplotype from a population. For each haplotype, the allelic states of the different loci are statistically dependent owing to genetic linkage, and for each locus, the allelic states of different haplotypes are statistically dependent owing to their shared ancestry. These dependencies are the result of the unique history of mutation, recombination and COALESCENCE of lineages in the ancestry of the sample (i.e., the merging of ancestral lineages going back in time). Population genetics is the study of the forces that create and maintain genetic variation. At the heart of it is the use of gene genealogy, a tree structure describes the evolutionary history (e.g., divergence) of a particular set of DNA sequences and their relatedness. Since the shape of the genealogy depends on population history, selection, etc, geneticists have been using it to infer or predict DNA sequence variation. Due to the uncertainty elements of reproduction, the central part of genealogical data analysis is a stochastic characterization of the genealogies that relate the sequences.



The two main reasons for wanting to simulate genetic data are, first, to gain insight into the effects that underlying demographic and mutational parameters may have on the genetic data one sees, and, secondly, to create test datasets for assessing the power of alternative genetic analysis methods. There is <a href="http://www.nature.com/nrg/journal/v13/n2/full/nrg3130.html">a recent review on this area</a> from a geneticist point of view. The article also points out several other review articles, which, according to the author, focus more on technical aspects.</p> The key challenges that all simulation algorithms face are: (1) speed — typically one wants to do lots of simulations, so they need to be fast; (2) scalability — with the advent of genome-wide genotyping and large-scale sequencing, there is a need for simulation programs to match; and (3) flexibility — can the program cope with different demographic histories, population structure, recombination, selection, mutation models and disease models? 

There are three main approaches to dealing with these challenges, here termed ‘backwards’, ‘forwards’ and ‘sideways’. The ‘backwards’ (or coalescent) approach (Kingman 1982, Hudson 1983 & 1990, Donnelly and Tavare 1995) is an efficient way to sample sequences from a theoretical population that follows the Wright-Fisher neutral model (Ewens 1979). It starts with the sample of individuals that will form your simulated dataset, then work backwards in time to construct the ancestral tree or graph of genealogical relationships that connects them all. Neutral mutations can subsequently be placed on this structure to create the simulated dataset. ‘Forwards’ simulations start with the entire population of individuals — typically, many thousands — and then follow how all the genetic data in question are passed on from one generation to the next. One usually needs to simulate over many thousands of generations in order to arrive at an equilibrium in which the genetic characteristics of the population are independent of the original starting conditions. By restricting attention just to the genealogical structure relevant to the sample in question, the coalescent approach is more efficient than the ‘forwards’ approach. Also, this approach usually employs a continuous-time approximation to effectively skip over the intermediate generations between important tree generating events, which makes it even more efficient.

Major statistics for evaluating the synthetic data and calibrationg the simulator
Linkage Disequilibrium (LD) 
Allele Frequency spectrum

As for future research interests, the simulation of <b>copy number variation</b> and/or microsatellite data at larger genomic scales, and of more complex disease models allowing covariates and linked loci, remain areas that deserve further exploration. (Note:  the performance of any methods trying to tackle the challenges of complex disease mapping should be evaluated by large scale simulation studies)

<h2>References</h2>
<ul>
<li>Liu, Youfang, Georgios Athanasiadis, and Michael E. Weale. "A survey of genetic simulation software for population and epidemiological studies." Hum Genomics 3.1 (2008): 79-86.</li>
<li>Hoban, Sean, Giorgio Bertorelle, and Oscar E. Gaggiotti. "Computer simulations: tools for population and evolutionary genetics." Nature Reviews Genetics 13.2 (2012): 110-122.</li>
<li>Rosenberg, Noah A., and Magnus Nordborg. "Genealogical trees, coalescent theory and the analysis of genetic polymorphisms." Nature Reviews Genetics 3.5 (2002): 380-390.</li>
</ul>
