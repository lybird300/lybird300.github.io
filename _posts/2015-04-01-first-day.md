---
layout: post
title: "The Very First Day"
date: 2015-04-01
---
<blockquote>All models are wrong. We make tentative assumptions about the real world which we know are false but which we believe may be useful.(Box 1976)</blockquote>

<p>Today I start poking into computer simulations for population and evolutionary genetics. The two main reasons for wanting to simulate genetic data are, first, to gain insight into the effects that underlying demographic and mutational parameters may have on the genetic data one sees, and, secondly, to create test datasets for assessing the power of alternative genetic analysis methods. There is <a href="http://www.nature.com/nrg/journal/v13/n2/full/nrg3130.html">a recent review on this area</a> from a geneticist point of view. The article also points out several other review articles, which, according to the author, focus more on technical aspects.</p>

In an effort to “make sense out of sequence”, modern evolutionary geneticists seek to use POLYMORPHISM DATA (i.e., data that include the genotypes of many individuals sampled at one or more loci; here we consider a locus to be polymorphic if two or more distinct types are observed, regardless of their frequencies) to test models about evolutionary forces. The modern approach that analyzes polymorphisms collected from natural populations has two fundamental problems. First, because there is no replication of the ‘experiment’, only one run of evolution is available to be studied, and second, the starting conditions of the ‘experiment’ are unknown. These problems might seem obvious, but it is not always appreciated how profound their implications are for data analysis. Consider a sample of HAPLOTYPES (the allelic configuration of multiple genetic markers that are present on a single chromosome of a given individual.) from a population. For each haplotype, the allelic states of the different loci are statistically dependent owing to genetic linkage, and for each locus, the allelic states of different haplotypes are statistically dependent owing to their shared ancestry. These dependencies are the result of the unique history of mutation, recombination and COALESCENCE of lineages in the ancestry of the sample (i.e., the merging of ancestral lineages going back in time). These facts must be incorporated if the data are to be analysed in a coherent statistical framework.Heuristic methods do not fully take into account the uncertainty caused by the inherent randomness of evolution, and as a result can lead to pronounced overinterpretations of the data. One solution is to model and simulate the past using a suitable stochastic model. The stochastic process known as ‘the coalescent’ (introduced in another post) is a natural extension of classical populationgenetics models and is very well suited for this purpose. It is relatively simple and can be adapted to accommodate a wide variety of biological assumptions.

The key challenges that all simulation algorithms face are: (1) speed — typically one wants to do lots of simulations, so they need to be fast; (2) scalability — with the advent of genome-wide genotyping and large-scale sequencing, there is a need for simulation programs to match; and (3) flexibility — can the program cope with different demographic histories, population structure, recombination, selection, mutation models and disease models? There are three main approaches to dealing with these challenges, here termed ‘backwards’, ‘forwards’ and ‘sideways’. The ‘backwards’ (or coalescent) approach (Kingman 1982, Hudson 1983 & 1990, Donnelly and Tavare 1995) is an efficient way to sample sequences from a theoretical population that follows the Wright-Fisher neutral model (Ewens 1979). It starts with the sample of individuals that will form your simulated dataset, then work backwards in time to construct the ancestral tree or graph of genealogical relationships that connects them all. Neutral mutations can subsequently be placed on this structure to create the simulated dataset. ‘Forwards’ simulations start with the entire population of individuals — typically, many thousands — and then follow how all the genetic data in question are passed on from one generation to the next. One usually needs to simulate over many thousands of generations in order to arrive at an equilibrium in which the genetic characteristics of the population are independent of the original starting conditions. By restricting attention just to the genealogical structure relevant to the sample in question, the coalescent approach is more efficient than the ‘forwards’ approach. Also, this approach usually employs a continuous-time approximation to effectively skip over the intermediate generations between important tree generating events, which makes it even more efficient.

Major statistics for evaluating the synthetic data and calibrationg the simulator
Linkage Disequilibrium (LD) patterns
Allele Frequency spectrum

As for future research interests, the simulation of <b>copy number variation</b> and/or microsatellite data at larger genomic scales, and of more complex disease models allowing covariates and linked loci, remain areas that deserve further exploration. (Note:  the performance of any methods trying to tackle the challenges of complex disease mapping should be evaluated by large scale simulation studies)

<h2>References</h2>
<ul>
<li>Liu, Youfang, Georgios Athanasiadis, and Michael E. Weale. "A survey of genetic simulation software for population and epidemiological studies." Hum Genomics 3.1 (2008): 79-86.</li>
<li>Hoban, Sean, Giorgio Bertorelle, and Oscar E. Gaggiotti. "Computer simulations: tools for population and evolutionary genetics." Nature Reviews Genetics 13.2 (2012): 110-122.</li>
</ul>
